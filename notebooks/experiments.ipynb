{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from datetime import date\n",
    "from typing import List, Set\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "DATE_LIMIT = date(2023, 12, 31)\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "USER_DATA_READ=f\"{BASE_PATH}/data/users-details-2023.csv\"\n",
    "USER_DATA_SAVE=f\"{BASE_PATH}/data/users.parquet\"\n",
    "\n",
    "ANIME_DATA_READ = f\"{BASE_PATH}/data/anime-dataset-2023.csv\"\n",
    "ANIME_DATA_SAVE = f\"{BASE_PATH}/data/animes.parquet\"\n",
    "\n",
    "SCORE_DATA_READ = f\"{BASE_PATH}/data/users-score-2023.csv\"\n",
    "SCORE_DATA_SAVE = f\"{BASE_PATH}/data/scores.parquet\"\n",
    "\n",
    "FINAL_DATASET_CUT6_BASIC_USER_DATA = f\"{BASE_PATH}/data/scores-cut6-basic.parquet\"\n",
    "FINAL_DATASET_CUT7_BASIC_USER_DATA = f\"{BASE_PATH}/data/scores-cut7-basic.parquet\"\n",
    "FINAL_DATASET_CUT8_BASIC_USER_DATA = f\"{BASE_PATH}/data/scores-cut8-basic.parquet\"\n",
    "\n",
    "FINAL_DATASET_CUT6_FULL_USER_DATA = f\"{BASE_PATH}/data/scores-cut6-full.parquet\"\n",
    "FINAL_DATASET_CUT7_FULL_USER_DATA = f\"{BASE_PATH}/data/scores-cut7-full.parquet\"\n",
    "FINAL_DATASET_CUT8_FULL_USER_DATA = f\"{BASE_PATH}/data/scores-cut8-full.parquet\"\n",
    "\n",
    "EXPERIMENT_LOG = f\"{BASE_PATH}/data/experiment-log.txt\"\n",
    "RESULTS_DIR = f\"{BASE_PATH}/data/results\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseReader:\n",
    "    def __init__(self, read_path: str, save_path: str):\n",
    "        self.file_path = read_path\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def to_parquet(self, df: pd.DataFrame) -> None:\n",
    "        df.to_parquet(self.save_path, index=False)\n",
    "\n",
    "    def get_stats(self, df: pd.DataFrame, columns: List[str]) -> dict:\n",
    "        result = dict()\n",
    "        for c in columns:\n",
    "            result[c] = {\n",
    "                \"hist\": df[c].value_counts(dropna=False).to_dict(),\n",
    "                \"max\": df[c].max(skipna=True) if df[c].dtype != \"O\" else 0,\n",
    "                \"mean\": df[c].mean(skipna=True) if df[c].dtype != \"O\" else 0,\n",
    "                \"median\": df[c].median(skipna=True) if df[c].dtype != \"O\" else 0,\n",
    "                \"min\": df[c].min(skipna=True) if df[c].dtype != \"O\" else 0\n",
    "            }\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def show_stats(self, result: dict) -> None:\n",
    "        for column in result.keys():\n",
    "            # Exibe estatísticas descritivas básicas\n",
    "            print(f\"Estatística descritiva de \\\"{column}\\\"\")\n",
    "            print(f\"Mínimo: {result[column][\"min\"]}\")\n",
    "            print(f\"Média: {result[column][\"mean\"]}\")\n",
    "            print(f\"Mediana: {result[column][\"median\"]}\")\n",
    "            print(f\"Máximo: {result[column][\"max\"]}\")\n",
    "\n",
    "            # Avalia a quantidade de nulos\n",
    "            count = 0\n",
    "            null = 0\n",
    "            for k in result[column][\"hist\"].keys():\n",
    "                count = count + result[column][\"hist\"][k]\n",
    "                if type(k) == float and np.isnan(k):\n",
    "                    null = result[column][\"hist\"][k]\n",
    "            percent = round(null * 100 / count, 2) if count > 0 else 0\n",
    "            print(f\"Quantidade de nulos: {null} ({percent}%)\")\n",
    "\n",
    "            # Exibe uma linha de separação\n",
    "            print(\"*\" * 40, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserReader(BaseReader):\n",
    "    def __init__(self, read_path: str, save_path: str):\n",
    "        super().__init__(read_path, save_path)\n",
    "\n",
    "    def first_process(self) -> pd.DataFrame:\n",
    "        # Carrega os dados, removendo colunas não utilizadas\n",
    "        remove_columns = [\n",
    "            \"Username\", \"Location\", \"Joined\",\n",
    "            \"On Hold\", \"Plan to Watch\", \"Rewatched\"\n",
    "        ]\n",
    "        df = pd.read_csv(self.file_path).drop(remove_columns, axis=1)\n",
    "\n",
    "        # Faz a troca de gênero definindo Male = 0 e Female = 1\n",
    "        def clear_gender(value: str) -> int:\n",
    "            if type(value) != str:\n",
    "                return None\n",
    "            return 0 if value.upper() == \"MALE\" else 1\n",
    "        df[\"Gender\"] = df[\"Gender\"].apply(clear_gender)\n",
    "\n",
    "        # Faz a conversão da data de nascimento na idade\n",
    "        def get_age(birth_date: str | float):\n",
    "            if type(birth_date) != str:\n",
    "                return None\n",
    "            return int((DATE_LIMIT - date.fromisoformat(birth_date.split(\"T\")[0])).days / 365)\n",
    "        df[\"age\"] = df[\"Birthday\"].apply(get_age)\n",
    "        df = df.drop([\"Birthday\"], axis=1)\n",
    "\n",
    "        # Faz a troca de nomes de colunas\n",
    "        df = df.rename(columns={\n",
    "            \"Mal ID\": \"user_id\",\n",
    "            \"Gender\": \"gender\",\n",
    "            \"Days Watched\": \"days_spent_with_anime\",\n",
    "            \"Mean Score\": \"mean_score\",\n",
    "            \"Watching\": \"current_anime_wathing\",\n",
    "            \"Completed\": \"total_anime_watched\",\n",
    "            \"Dropped\": \"dropped_anime\",\n",
    "            \"Total Entries\": \"anime_in_list\",\n",
    "            \"Episodes Watched\": \"episodes_watched\"\n",
    "        })\n",
    "\n",
    "        # Salva o arquivo limpo\n",
    "        return df\n",
    "\n",
    "    def remove_nulls(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        original_rows = len(df)\n",
    "        df = df.dropna()\n",
    "        new_rows = len(df)\n",
    "        percent = round((original_rows - new_rows) * 100 / original_rows, 2)\n",
    "        print(f\"Remoção de {original_rows - new_rows} linhas ({percent}%)\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_user_analysis():\n",
    "    reader = UserReader(USER_DATA_READ, USER_DATA_SAVE)\n",
    "    df_user = reader.first_process()\n",
    "\n",
    "    stats = reader.get_stats(\n",
    "        df_user,\n",
    "        [\n",
    "            \"gender\", \"days_spent_with_anime\", \"mean_score\",\n",
    "            \"current_anime_wathing\", \"total_anime_watched\",\n",
    "            \"dropped_anime\", \"anime_in_list\", \"episodes_watched\", \"age\"\n",
    "        ]\n",
    "    )\n",
    "    reader.show_stats(stats)\n",
    "\n",
    "    df_user = reader.remove_nulls(df_user)\n",
    "    reader.to_parquet(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_user_analysis()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeReader(BaseReader):\n",
    "    def __init__(self, read_path: str, save_path: str):\n",
    "        super().__init__(read_path, save_path)\n",
    "\n",
    "    def first_process(self) -> pd.DataFrame:\n",
    "        # Carrega dados\n",
    "        df = pd.read_csv(self.file_path)\n",
    "\n",
    "        # Remove colunas não utilizadas\n",
    "        use_columns = [\"anime_id\", \"Genres\", \"Episodes\", \"Source\", \"Duration\"]\n",
    "        df = df[use_columns]\n",
    "\n",
    "        # Faz a conversão do texto de duração para o valor numérico\n",
    "        def extract_duration(description: str):\n",
    "            if description.upper() == \"UNKNOWN\":\n",
    "                return np.nan\n",
    "            numbers = re.findall(r\"[0-9]+\", description)\n",
    "            if len(numbers) == 2:\n",
    "                return int(numbers[0]) * 60 + int(numbers[1])\n",
    "            else:\n",
    "                return int(numbers[0])\n",
    "        df[\"Duration\"] = df[\"Duration\"].apply(extract_duration)\n",
    "\n",
    "        # Converte o número de episódios em números e remove nulos\n",
    "        df[\"Episodes\"] = df[\"Episodes\"].apply(lambda x: float(x) if x.upper() != \"UNKNOWN\" else np.nan).astype(\"float64\")\n",
    "\n",
    "        # Aplica uma padronização nos nomes dos materiais originais\n",
    "        def standard_source(source: str):\n",
    "            conv_source = {\n",
    "                \"4-koma manga\": \"manga\",\n",
    "                \"Book\": \"book\",\n",
    "                \"Card game\": \"game\",\n",
    "                \"Game\": \"game\",\n",
    "                \"Light novel\": \"novel\",\n",
    "                \"Manga\": \"manga\",\n",
    "                \"Mixed media\": \"other\",\n",
    "                \"Music\": \"other\",\n",
    "                \"Novel\": \"novel\",\n",
    "                \"Original\": \"original\",\n",
    "                \"Other\": \"other\",\n",
    "                \"Picture book\": \"other\",\n",
    "                \"Radio\": \"other\",\n",
    "                \"Unknown\": np.nan,\n",
    "                \"Visual novel\": \"visual_novel\",\n",
    "                \"Web manga\": \"manga\",\n",
    "                \"Web novel\": \"novel\"\n",
    "            }\n",
    "            try:\n",
    "                return conv_source[source]\n",
    "            except:\n",
    "                return np.nan\n",
    "        df[\"Source\"] = df[\"Source\"].apply(standard_source)\n",
    "\n",
    "        # Resolve nomenclatura de gêneros\n",
    "        df[\"Genres\"] = df[\"Genres\"].apply(lambda x: np.nan if x == \"UNKNOWN\" else x)\n",
    "\n",
    "        # Faz a troca dos nomes das colunas\n",
    "        df = df.rename(columns={\n",
    "            \"Genres\": \"genres\",\n",
    "            \"Episodes\": \"episodes\",\n",
    "            \"Source\": \"source\",\n",
    "            \"Duration\": \"duration\"\n",
    "        })\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def remove_nulls(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.dropna(subset=[\"source\", \"duration\", \"episodes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_anime_analysis():\n",
    "    anime_reader = AnimeReader(ANIME_DATA_READ, ANIME_DATA_SAVE)\n",
    "\n",
    "    df_anime = anime_reader.first_process()\n",
    "    stats = anime_reader.get_stats(\n",
    "        df_anime,\n",
    "        [\"genres\", \"episodes\", \"source\", \"duration\"]\n",
    "    )\n",
    "    anime_reader.show_stats(stats)\n",
    "    df_anime = anime_reader.remove_nulls(df_anime)\n",
    "    anime_reader.to_parquet(df_anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_anime_analysis()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreReader(BaseReader):\n",
    "    def __init__(self, read_path: str, save_path: str, user_path: str, anime_path: str):\n",
    "        super().__init__(read_path, save_path)\n",
    "        self.anime_path = anime_path\n",
    "        self.user_path = user_path\n",
    "\n",
    "    def make_dataset(self, rating_cut=7, user_merge_mode=1) -> pd.DataFrame:\n",
    "        # Verifica integridade dos parâmetros\n",
    "        if rating_cut > 10 or rating_cut < 1:\n",
    "            raise Exception(\"O corte da classificação deve ser entre 1 e 10\")\n",
    "        \n",
    "        if user_merge_mode not in [1, 2]:\n",
    "            raise Exception(\"O modo de merge de usuário deve ser 1 ou 2\")\n",
    "        \n",
    "        # Carrega os dados dos scores, limpando as colunas não utilizadas\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        df = df.drop([\"Username\", \"Anime Title\"], axis=1)\n",
    "\n",
    "        # Carrega os dados de usuários e animes\n",
    "        users = pd.read_parquet(self.user_path)\n",
    "        animes = pd.read_parquet(self.anime_path)\n",
    "\n",
    "        # Recupera todos os gêneros possíveis\n",
    "        genres = [[s.strip() for s in g.split(\",\")] for g in animes[\"genres\"].values if g is not None]\n",
    "        genres: Set[str] = set(itertools.chain.from_iterable(genres))\n",
    "\n",
    "        # Define a função de verificação de gênero\n",
    "        # Os dados de gêneros são carregados como uma string,\n",
    "        # com as categorias separadas por vírgula\n",
    "        def verify_genre(genres: str | None, genre: str) -> int:\n",
    "            if genres is None:\n",
    "                return 0\n",
    "            \n",
    "            genres = [s.lower().strip() for s in genres.split(\",\")]\n",
    "            return 1 if genre.lower() in genres else 0\n",
    "\n",
    "        # Aplica o encoder para gêneros de animes\n",
    "        for genre in genres:\n",
    "            column = f\"genre_{\"_\".join(genre.lower().split(\" \"))}\"\n",
    "            animes[column] = animes[\"genres\"].apply(lambda x: verify_genre(x, genre))\n",
    "        animes = animes.drop([\"genres\"], axis=1)\n",
    "\n",
    "        # Define um encoder para o material original do anime\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "        encoder.fit(animes[[\"source\"]])\n",
    "\n",
    "        # Atualiza os dados de anime com o encoder de material original\n",
    "        encoder_df = pd.DataFrame(\n",
    "            encoder.transform(animes[[\"source\"]]),\n",
    "            columns=encoder.get_feature_names_out()\n",
    "        )\n",
    "        animes = pd.concat((animes, encoder_df), axis=1)\n",
    "        animes = animes.drop([\"source\"], axis=1)\n",
    "\n",
    "        # Executa o merge com os dados de usuários\n",
    "        # user_merge_mode = 1 faz com que apenas os dados básicos sejam usados\n",
    "        # user_merge_mode = 2 utiliza todos os dados de usuários\n",
    "        if user_merge_mode == 1:\n",
    "            users = users[[\"user_id\", \"gender\", \"age\"]]\n",
    "\n",
    "        if user_merge_mode == 2:\n",
    "            users = users[[\"user_id\", \"gender\", \"age\", \"days_spent_with_anime\", \"total_anime_watched\", \"dropped_anime\", \"mean_score\"]]\n",
    "        \n",
    "        df = df.merge(users, how=\"inner\", on=\"user_id\")\n",
    "\n",
    "        # Executa o merge com os dados de animes\n",
    "        df = df.merge(animes, how=\"inner\", on=\"anime_id\")\n",
    "\n",
    "        # Faz a criação da coluna target\n",
    "        df[\"target\"] = df[\"rating\"].apply(lambda x: 1 if x > rating_cut else 0)\n",
    "        df = df.drop([\"rating\"], axis=1)\n",
    "\n",
    "        # Finaliza o processo, removendo colunas de ID\n",
    "        df = df.drop([\"user_id\", \"anime_id\"], axis=1)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets():\n",
    "    result_files = [\n",
    "        FINAL_DATASET_CUT6_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT7_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT8_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT6_FULL_USER_DATA,\n",
    "        FINAL_DATASET_CUT7_FULL_USER_DATA,\n",
    "        FINAL_DATASET_CUT8_FULL_USER_DATA\n",
    "    ]\n",
    "\n",
    "    for save_path in result_files:\n",
    "        score_reader = ScoreReader(\n",
    "            SCORE_DATA_READ,\n",
    "            save_path,\n",
    "            USER_DATA_SAVE,\n",
    "            ANIME_DATA_SAVE\n",
    "        )\n",
    "        scores = score_reader.make_dataset()\n",
    "        score_reader.to_parquet(scores)\n",
    "\n",
    "        del scores\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_datasets()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4Layers(nn.Module):\n",
    "    def __init__(self, n_features: int, n_classes=2, n_neurons=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_neurons)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc4 = nn.Linear(n_neurons, n_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.out = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Model8Layers(nn.Module):\n",
    "    def __init__(self, n_features: int, n_classes=2, n_neurons=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_neurons)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc4 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc5 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc6 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc7 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc8 = nn.Linear(n_neurons, n_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.out = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manager:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_dataset(self, read_path: str, sample=0.2):\n",
    "        # Carrega dados processados\n",
    "        df = pd.read_parquet(read_path).sample(frac=sample)\n",
    "        X = df.drop([\"target\"], axis=1).values\n",
    "        y = df[\"target\"].values\n",
    "\n",
    "        # Faz a divisão entre treino e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        # Aplica a padronização de valores\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Faz a transformação de numpy array para tensor\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        # Instancia dataset de tensores\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "        # Instancia loader de tensores\n",
    "        train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "        return train_dataset, test_dataset, train_loader, test_loader\n",
    "    \n",
    "    def execute(self, train_dataset: DataLoader, train_loader: DataLoader, epochs=100, n_neurons=16, arch=1):\n",
    "        # Garante consistência da arquitetura\n",
    "        if arch not in [1, 2]:\n",
    "            raise Exception(\"As arquiteturas válidas são 1 e 2\")\n",
    "        \n",
    "        # Regitra o tempo de início do treinamento\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Carrega dados e inicializa modelo\n",
    "        classes_ = len(train_dataset.tensors[1].unique())\n",
    "        if arch == 1:\n",
    "            model = Model4Layers(\n",
    "                n_features=train_dataset.tensors[0].shape[1],\n",
    "                n_classes=classes_,\n",
    "                n_neurons=n_neurons\n",
    "            )\n",
    "        elif arch == 2:\n",
    "            model = Model8Layers(\n",
    "                n_features=train_dataset.tensors[0].shape[1],\n",
    "                n_classes=classes_,\n",
    "                n_neurons=n_neurons\n",
    "            )\n",
    "\n",
    "        # Define o modo de otimização\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(0, epochs):\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            model.train()\n",
    "            count_batch = 0\n",
    "            limit_batch = (train_dataset.tensors[0].shape[0] // train_loader.batch_size) + 1\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                percent = round(count_batch * 100 / limit_batch, 2)\n",
    "                print(f\"Epoch {epoch + 1} Batch {count_batch + 1} ({percent}%)\", end=\"\\r\")\n",
    "                inputs = inputs\n",
    "                labels = labels\n",
    "\n",
    "                # Inicia os gradientes e calcula a predição\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                pred_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                # Teoricamente, seria preciso passar as labels do dataset para o\n",
    "                # padrão one hot encoder, porém a camada softmax no modelo já\n",
    "                # resolve isso.\n",
    "                # oh_labels = F.one_hot(labels.long())\n",
    "                # loss = criterion(outputs, torch.reshape(oh_labels, (oh_labels.size()[0], classes_)).float())\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Calcula os gradientes e atualiza os pesos\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Fal a atualização das estatísticas de acompanhamento\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(pred_labels == labels.data).item()\n",
    "                count_batch = count_batch + 1\n",
    "\n",
    "            # Exibe estatísticas de acompanhamento\n",
    "            num_samples = len(train_dataset)\n",
    "            epoch_loss = running_loss / num_samples\n",
    "            epoch_accuracy = running_corrects / num_samples\n",
    "            print(f\"Epoch {epoch + 1}: Loss {epoch_loss:.3f} Acurácia {epoch_accuracy:.3f}\")\n",
    "\n",
    "        # Calcula o tempo de execução do treinamento\n",
    "        end_time = time.time()\n",
    "        train_time = end_time - start_time\n",
    "\n",
    "        return model, train_time, epoch_loss\n",
    "    \n",
    "    def compute_test(self, model: nn.Module, test_loader: DataLoader, train_time: float, train_loss: float) -> dict:\n",
    "        # Define modelo como avaliação e inicia as listas de labels\n",
    "        model.eval()\n",
    "        pred_labels_all = []\n",
    "        true_labels_all = []\n",
    "\n",
    "        # Passa pelo loader para cálculo das predições\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            pred_labels = torch.argmax(outputs, dim=1)\n",
    "            pred_labels_all.append(pred_labels)\n",
    "            true_labels_all.append(labels)\n",
    "\n",
    "        # Concatena os resultados\n",
    "        pred_labels = torch.cat(pred_labels_all, dim=0).cpu().numpy()\n",
    "        true_labels = torch.cat(true_labels_all, dim=0).numpy()\n",
    "\n",
    "        # Registra dados no dicionário de dados\n",
    "        return {\n",
    "            \"train_time\": train_time,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"metrics\": {\n",
    "                \"accuracy\": (pred_labels == true_labels).mean(),\n",
    "                \"f1-score\": f1_score(true_labels, pred_labels, pos_label=1, average=\"binary\"),\n",
    "                \"recall-score\": recall_score(true_labels, pred_labels, pos_label=1, average=\"binary\"),\n",
    "                \"precission-score\": precision_score(true_labels, pred_labels, pos_label=1, average=\"binary\")\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_experiments():\n",
    "    # Define os parâmetros dos experimentos\n",
    "    archs_set = [1, 2]\n",
    "    neurons_set = [16]\n",
    "    sample_data = [0.1, 0.2]\n",
    "    epochs = [10]\n",
    "    repeat = 5\n",
    "    data_paths = [\n",
    "        FINAL_DATASET_CUT6_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT7_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT8_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT6_FULL_USER_DATA,\n",
    "        FINAL_DATASET_CUT7_FULL_USER_DATA,\n",
    "        FINAL_DATASET_CUT8_FULL_USER_DATA\n",
    "    ]\n",
    "\n",
    "    # Verifica o log de experimentos\n",
    "    if not os.path.exists(EXPERIMENT_LOG):\n",
    "        with open(EXPERIMENT_LOG, \"w\") as file:\n",
    "            file.write(\"dataset_type,arch,neurons,sample,epochs,iteration,weight_file,predict_file\\n\")\n",
    "\n",
    "    # Função auxiliar: abre o log e verifica registros\n",
    "    def verify(dataset_type: str, arch: int, neurons: int, sample: float, epochs: int, iteration: int):\n",
    "        exist = False\n",
    "        with open(EXPERIMENT_LOG, \"r\") as file:\n",
    "            row = file.readline()\n",
    "            while row:\n",
    "                row_dataset_type, row_arch, row_neurons, row_sample, row_epochs, row_iteration, _, _ = row.split(\",\")\n",
    "                row_params = [row_dataset_type, row_arch, row_neurons, row_sample, row_epochs, row_iteration]\n",
    "                search_params = [str(dataset_type), str(arch), str(neurons), str(sample), str(epochs), str(iteration)]\n",
    "                #print(row_params, search_params)\n",
    "                \n",
    "                if row_params == search_params:\n",
    "                    exist = True\n",
    "                    break\n",
    "\n",
    "                row = file.readline()\n",
    "\n",
    "        return exist\n",
    "\n",
    "    for data_path in data_paths:\n",
    "        # Tipo de dataset utilizado\n",
    "        dataset_type = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        for neurons in neurons_set:\n",
    "            # Quantidade de neurônios das camadas internas\n",
    "\n",
    "            for sample in sample_data:\n",
    "                # Porção dos dados fracionados\n",
    "\n",
    "                for epoch in epochs:\n",
    "                    # Quantidade de épocas do treinamento\n",
    "\n",
    "                    for arch in archs_set:\n",
    "                        # Profundidade da rede\n",
    "\n",
    "                        for i in range(0, repeat):\n",
    "                            # Verifica se o experimento já foi executado\n",
    "                            if verify(dataset_type, arch, neurons, sample, epoch, i):\n",
    "                                continue\n",
    "\n",
    "                            # Registra todos os dados do experimento\n",
    "                            unique_name = int(time.time())\n",
    "                            weight_path = f\"{RESULTS_DIR}/{unique_name}.pth\"\n",
    "                            predict_path = f\"{RESULTS_DIR}/{unique_name}.json\"\n",
    "                            experiment_data = [\n",
    "                                    dataset_type,\n",
    "                                    str(arch),\n",
    "                                    str(neurons),\n",
    "                                    str(sample),\n",
    "                                    str(epoch),\n",
    "                                    str(i),\n",
    "                                    f\"{unique_name}.pth\",\n",
    "                                    f\"{unique_name}.json\"\n",
    "                                ]\n",
    "\n",
    "                            # Log de execução\n",
    "                            print(f\"Execução do experimento {\",\".join(experiment_data[:-2])}\".upper())\n",
    "\n",
    "                            # Repetição do experimento\n",
    "                            process = Manager()\n",
    "                            train_dataset, test_dataset, train_loader, test_loader = process.get_dataset(data_path, sample=sample)\n",
    "                            gc.collect()\n",
    "                            model, train_time, train_loss = process.execute(train_dataset, train_loader, epochs=epoch, n_neurons=neurons, arch=arch)\n",
    "\n",
    "                            # Executa o teste do modelo\n",
    "                            results = process.compute_test(model, test_loader, train_time, train_loss)\n",
    "                            \n",
    "                            # Salva o json de métricas\n",
    "                            with open(predict_path, \"w+\") as file:\n",
    "                                file.write(json.dumps(results))\n",
    "\n",
    "                            # Salva os pesos do modelo\n",
    "                            torch.save(model.state_dict(), weight_path)\n",
    "\n",
    "                            # Registra no log\n",
    "                            with open(EXPERIMENT_LOG, \"a\") as file:\n",
    "                                file.write(\",\".join(experiment_data) + \"\\n\")\n",
    "\n",
    "                            # Libera memória\n",
    "                            del train_dataset, test_dataset, train_loader, test_loader\n",
    "                            gc.collect()\n",
    "                            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>arch</th>\n",
       "      <th>neurons</th>\n",
       "      <th>sample</th>\n",
       "      <th>epochs</th>\n",
       "      <th>iteration</th>\n",
       "      <th>weight_file</th>\n",
       "      <th>predict_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scores-cut6-basic</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1731500580.pth</td>\n",
       "      <td>1731500580.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scores-cut6-basic</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1731500656.pth</td>\n",
       "      <td>1731500656.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scores-cut6-basic</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1731500728.pth</td>\n",
       "      <td>1731500728.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scores-cut6-basic</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1731500799.pth</td>\n",
       "      <td>1731500799.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scores-cut6-basic</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1731500872.pth</td>\n",
       "      <td>1731500872.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>scores-cut8-full</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1731513051.pth</td>\n",
       "      <td>1731513051.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>scores-cut8-full</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1731513205.pth</td>\n",
       "      <td>1731513205.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>scores-cut8-full</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1731513355.pth</td>\n",
       "      <td>1731513355.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>scores-cut8-full</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1731513505.pth</td>\n",
       "      <td>1731513505.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>scores-cut8-full</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1731513658.pth</td>\n",
       "      <td>1731513658.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          dataset_type  arch  neurons  sample  epochs  iteration  \\\n",
       "0    scores-cut6-basic     1       16     0.1      10          0   \n",
       "1    scores-cut6-basic     1       16     0.1      10          1   \n",
       "2    scores-cut6-basic     1       16     0.1      10          2   \n",
       "3    scores-cut6-basic     1       16     0.1      10          3   \n",
       "4    scores-cut6-basic     1       16     0.1      10          4   \n",
       "..                 ...   ...      ...     ...     ...        ...   \n",
       "115   scores-cut8-full     2       16     0.2      10          0   \n",
       "116   scores-cut8-full     2       16     0.2      10          1   \n",
       "117   scores-cut8-full     2       16     0.2      10          2   \n",
       "118   scores-cut8-full     2       16     0.2      10          3   \n",
       "119   scores-cut8-full     2       16     0.2      10          4   \n",
       "\n",
       "        weight_file     predict_file  \n",
       "0    1731500580.pth  1731500580.json  \n",
       "1    1731500656.pth  1731500656.json  \n",
       "2    1731500728.pth  1731500728.json  \n",
       "3    1731500799.pth  1731500799.json  \n",
       "4    1731500872.pth  1731500872.json  \n",
       "..              ...              ...  \n",
       "115  1731513051.pth  1731513051.json  \n",
       "116  1731513205.pth  1731513205.json  \n",
       "117  1731513355.pth  1731513355.json  \n",
       "118  1731513505.pth  1731513505.json  \n",
       "119  1731513658.pth  1731513658.json  \n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(EXPERIMENT_LOG)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeira comparação: existe diferença entre utilizar dados básicos e metadados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor</th>\n",
       "      <th>basic_f1</th>\n",
       "      <th>full_f1</th>\n",
       "      <th>(full-basic) %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cut8-0.2-2</td>\n",
       "      <td>0.707960</td>\n",
       "      <td>0.705540</td>\n",
       "      <td>-0.341773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cut7-0.1-1</td>\n",
       "      <td>0.706680</td>\n",
       "      <td>0.704263</td>\n",
       "      <td>-0.342012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cut7-0.2-1</td>\n",
       "      <td>0.706703</td>\n",
       "      <td>0.704651</td>\n",
       "      <td>-0.290471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cut8-0.1-2</td>\n",
       "      <td>0.707420</td>\n",
       "      <td>0.703235</td>\n",
       "      <td>-0.591593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cut6-0.1-2</td>\n",
       "      <td>0.707292</td>\n",
       "      <td>0.700698</td>\n",
       "      <td>-0.932328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cut7-0.2-2</td>\n",
       "      <td>0.705210</td>\n",
       "      <td>0.709543</td>\n",
       "      <td>0.614460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cut8-0.2-1</td>\n",
       "      <td>0.707439</td>\n",
       "      <td>0.707936</td>\n",
       "      <td>0.070331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cut6-0.1-1</td>\n",
       "      <td>0.703058</td>\n",
       "      <td>0.709564</td>\n",
       "      <td>0.925313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cut6-0.2-1</td>\n",
       "      <td>0.707050</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>-0.399524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cut7-0.1-2</td>\n",
       "      <td>0.700951</td>\n",
       "      <td>0.705855</td>\n",
       "      <td>0.699642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cut6-0.2-2</td>\n",
       "      <td>0.702501</td>\n",
       "      <td>0.706176</td>\n",
       "      <td>0.523220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cut8-0.1-1</td>\n",
       "      <td>0.703752</td>\n",
       "      <td>0.708675</td>\n",
       "      <td>0.699624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        factor  basic_f1   full_f1  (full-basic) %\n",
       "0   cut8-0.2-2  0.707960  0.705540       -0.341773\n",
       "1   cut7-0.1-1  0.706680  0.704263       -0.342012\n",
       "2   cut7-0.2-1  0.706703  0.704651       -0.290471\n",
       "3   cut8-0.1-2  0.707420  0.703235       -0.591593\n",
       "4   cut6-0.1-2  0.707292  0.700698       -0.932328\n",
       "5   cut7-0.2-2  0.705210  0.709543        0.614460\n",
       "6   cut8-0.2-1  0.707439  0.707936        0.070331\n",
       "7   cut6-0.1-1  0.703058  0.709564        0.925313\n",
       "8   cut6-0.2-1  0.707050  0.704225       -0.399524\n",
       "9   cut7-0.1-2  0.700951  0.705855        0.699642\n",
       "10  cut6-0.2-2  0.702501  0.706176        0.523220\n",
       "11  cut8-0.1-1  0.703752  0.708675        0.699624"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descreve fatores de análise\n",
    "factors: Set[str] = set()\n",
    "for dataset_type in df[\"dataset_type\"].unique():\n",
    "    for arch in df[\"arch\"].unique():\n",
    "        for sample in df[\"sample\"].unique():\n",
    "            dt = dataset_type.split(\"-\")[1]\n",
    "            factors.add(f\"{dt}-{sample}-{arch}\")\n",
    "\n",
    "# Calcula as médias para cada fator\n",
    "metrics = []\n",
    "for factor in factors:\n",
    "    dt, sample, arch = factor.split(\"-\")\n",
    "    basic = f\"scores-{dt}-basic\"\n",
    "    full = f\"scores-{dt}-full\"\n",
    "\n",
    "    metric = {\n",
    "        \"factor\": factor,\n",
    "        \"basic_f1\": [],\n",
    "        \"full_f1\": []\n",
    "    }\n",
    "\n",
    "    subset = df.loc[\n",
    "        (df[\"sample\"] == float(sample))\n",
    "        & (df[\"arch\"] == int(arch))\n",
    "    ]\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        with open(f\"{RESULTS_DIR}/{row[\"predict_file\"]}\") as file:\n",
    "            result = json.load(file)\n",
    "            \n",
    "        if row[\"dataset_type\"] == basic:\n",
    "            metric[\"basic_f1\"].append(result[\"metrics\"][\"f1-score\"])\n",
    "\n",
    "        elif row[\"dataset_type\"] == full:\n",
    "            metric[\"full_f1\"].append(result[\"metrics\"][\"f1-score\"])\n",
    "\n",
    "    metric[\"basic_f1\"] = np.mean(metric[\"basic_f1\"])\n",
    "    metric[\"full_f1\"] = np.mean(metric[\"full_f1\"])\n",
    "    metric[\"(full-basic) %\"] = (metric[\"full_f1\"] - metric[\"basic_f1\"]) * 100 / metric[\"basic_f1\"]\n",
    "\n",
    "    metrics.append(metric)\n",
    "\n",
    "metrics = pd.DataFrame(metrics)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segunda comparação: existe diferença de onde fazer o corte de gostou/desgostou?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor</th>\n",
       "      <th>cut6</th>\n",
       "      <th>cut7</th>\n",
       "      <th>cut8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full-0.1-2</td>\n",
       "      <td>0.700698</td>\n",
       "      <td>0.705855</td>\n",
       "      <td>0.703235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basic-0.1-2</td>\n",
       "      <td>0.707292</td>\n",
       "      <td>0.700951</td>\n",
       "      <td>0.707420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>basic-0.2-2</td>\n",
       "      <td>0.702501</td>\n",
       "      <td>0.705210</td>\n",
       "      <td>0.707960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic-0.2-1</td>\n",
       "      <td>0.707050</td>\n",
       "      <td>0.706703</td>\n",
       "      <td>0.707439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>full-0.2-2</td>\n",
       "      <td>0.706176</td>\n",
       "      <td>0.709543</td>\n",
       "      <td>0.705540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>full-0.2-1</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.704651</td>\n",
       "      <td>0.707936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>basic-0.1-1</td>\n",
       "      <td>0.703058</td>\n",
       "      <td>0.706680</td>\n",
       "      <td>0.703752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>full-0.1-1</td>\n",
       "      <td>0.709564</td>\n",
       "      <td>0.704263</td>\n",
       "      <td>0.708675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        factor      cut6      cut7      cut8\n",
       "0   full-0.1-2  0.700698  0.705855  0.703235\n",
       "1  basic-0.1-2  0.707292  0.700951  0.707420\n",
       "2  basic-0.2-2  0.702501  0.705210  0.707960\n",
       "3  basic-0.2-1  0.707050  0.706703  0.707439\n",
       "4   full-0.2-2  0.706176  0.709543  0.705540\n",
       "5   full-0.2-1  0.704225  0.704651  0.707936\n",
       "6  basic-0.1-1  0.703058  0.706680  0.703752\n",
       "7   full-0.1-1  0.709564  0.704263  0.708675"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descreve fatores de análise\n",
    "factors: Set[str] = set()\n",
    "for dataset_type in df[\"dataset_type\"].unique():\n",
    "    for arch in df[\"arch\"].unique():\n",
    "        for sample in df[\"sample\"].unique():\n",
    "            dt = dataset_type.split(\"-\")[2]\n",
    "            factors.add(f\"{dt}-{sample}-{arch}\")\n",
    "\n",
    "# Calcula as médias para cada fator\n",
    "metrics = []\n",
    "for factor in factors:\n",
    "    dt, sample, arch = factor.split(\"-\")\n",
    "    cut6 = f\"scores-cut6-{dt}\"\n",
    "    cut7 = f\"scores-cut7-{dt}\"\n",
    "    cut8 = f\"scores-cut8-{dt}\"\n",
    "\n",
    "    metric = {\n",
    "        \"factor\": factor,\n",
    "        \"cut6\": [],\n",
    "        \"cut7\": [],\n",
    "        \"cut8\": []\n",
    "    }\n",
    "\n",
    "    subset = df.loc[\n",
    "        (df[\"sample\"] == float(sample))\n",
    "        & (df[\"arch\"] == int(arch))\n",
    "    ]\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        with open(f\"{RESULTS_DIR}/{row[\"predict_file\"]}\") as file:\n",
    "            result = json.load(file)\n",
    "            \n",
    "        if row[\"dataset_type\"] == cut6:\n",
    "            metric[\"cut6\"].append(result[\"metrics\"][\"f1-score\"])\n",
    "\n",
    "        if row[\"dataset_type\"] == cut7:\n",
    "            metric[\"cut7\"].append(result[\"metrics\"][\"f1-score\"])\n",
    "\n",
    "        if row[\"dataset_type\"] == cut8:\n",
    "            metric[\"cut8\"].append(result[\"metrics\"][\"f1-score\"])\n",
    "\n",
    "    metric[\"cut6\"] = np.mean(metric[\"cut6\"])\n",
    "    metric[\"cut7\"] = np.mean(metric[\"cut7\"])\n",
    "    metric[\"cut8\"] = np.mean(metric[\"cut8\"])\n",
    "\n",
    "    metrics.append(metric)\n",
    "\n",
    "metrics = pd.DataFrame(metrics)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terceira comparação: existe diferença entre abordagens com diferentes profundidades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor</th>\n",
       "      <th>arch1</th>\n",
       "      <th>arch2</th>\n",
       "      <th>(arch2-arch1) %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scores-cut6-basic/0.1</td>\n",
       "      <td>0.703058</td>\n",
       "      <td>0.707292</td>\n",
       "      <td>0.602211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scores-cut7-basic/0.1</td>\n",
       "      <td>0.706680</td>\n",
       "      <td>0.700951</td>\n",
       "      <td>-0.810680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scores-cut8-basic/0.2</td>\n",
       "      <td>0.707439</td>\n",
       "      <td>0.707960</td>\n",
       "      <td>0.073630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scores-cut6-full/0.1</td>\n",
       "      <td>0.709564</td>\n",
       "      <td>0.700698</td>\n",
       "      <td>-1.249483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scores-cut8-basic/0.1</td>\n",
       "      <td>0.703752</td>\n",
       "      <td>0.707420</td>\n",
       "      <td>0.521220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>scores-cut8-full/0.1</td>\n",
       "      <td>0.708675</td>\n",
       "      <td>0.703235</td>\n",
       "      <td>-0.767709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>scores-cut7-basic/0.2</td>\n",
       "      <td>0.706703</td>\n",
       "      <td>0.705210</td>\n",
       "      <td>-0.211295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>scores-cut7-full/0.2</td>\n",
       "      <td>0.704651</td>\n",
       "      <td>0.709543</td>\n",
       "      <td>0.694354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>scores-cut6-full/0.2</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.706176</td>\n",
       "      <td>0.277121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>scores-cut6-basic/0.2</td>\n",
       "      <td>0.707050</td>\n",
       "      <td>0.702501</td>\n",
       "      <td>-0.643364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>scores-cut8-full/0.2</td>\n",
       "      <td>0.707936</td>\n",
       "      <td>0.705540</td>\n",
       "      <td>-0.338487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>scores-cut7-full/0.1</td>\n",
       "      <td>0.704263</td>\n",
       "      <td>0.705855</td>\n",
       "      <td>0.226076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   factor     arch1     arch2  (arch2-arch1) %\n",
       "0   scores-cut6-basic/0.1  0.703058  0.707292         0.602211\n",
       "1   scores-cut7-basic/0.1  0.706680  0.700951        -0.810680\n",
       "2   scores-cut8-basic/0.2  0.707439  0.707960         0.073630\n",
       "3    scores-cut6-full/0.1  0.709564  0.700698        -1.249483\n",
       "4   scores-cut8-basic/0.1  0.703752  0.707420         0.521220\n",
       "5    scores-cut8-full/0.1  0.708675  0.703235        -0.767709\n",
       "6   scores-cut7-basic/0.2  0.706703  0.705210        -0.211295\n",
       "7    scores-cut7-full/0.2  0.704651  0.709543         0.694354\n",
       "8    scores-cut6-full/0.2  0.704225  0.706176         0.277121\n",
       "9   scores-cut6-basic/0.2  0.707050  0.702501        -0.643364\n",
       "10   scores-cut8-full/0.2  0.707936  0.705540        -0.338487\n",
       "11   scores-cut7-full/0.1  0.704263  0.705855         0.226076"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descreve fatores de análise\n",
    "factors: Set[str] = set()\n",
    "for dataset_type in df[\"dataset_type\"].unique():\n",
    "    for sample in df[\"sample\"].unique():\n",
    "        factors.add(f\"{dataset_type}/{sample}\")\n",
    "\n",
    "# Calcula as médias para cada fator\n",
    "metrics = []\n",
    "for factor in factors:\n",
    "    dt, sample = factor.split(\"/\")\n",
    "    arch1, arch2 = 1, 2\n",
    "\n",
    "    metric = {\n",
    "        \"factor\": factor,\n",
    "        \"arch1\": [],\n",
    "        \"arch2\": []\n",
    "    }\n",
    "\n",
    "    subset = df.loc[\n",
    "        (df[\"sample\"] == float(sample))\n",
    "        & (df[\"dataset_type\"] == dt)\n",
    "    ]\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        with open(f\"{RESULTS_DIR}/{row[\"predict_file\"]}\") as file:\n",
    "            result = json.load(file)\n",
    "            \n",
    "        if row[\"arch\"] == arch1:\n",
    "            metric[\"arch1\"].append(result[\"metrics\"][\"f1-score\"])\n",
    "\n",
    "        if row[\"arch\"] == arch2:\n",
    "            metric[\"arch2\"].append(result[\"metrics\"][\"f1-score\"])\n",
    "\n",
    "    metric[\"arch1\"] = np.mean(metric[\"arch1\"])\n",
    "    metric[\"arch2\"] = np.mean(metric[\"arch2\"])\n",
    "    metric[\"(arch2-arch1) %\"] = (metric[\"arch2\"] - metric[\"arch1\"]) * 100 / metric[\"arch1\"]\n",
    "\n",
    "    metrics.append(metric)\n",
    "\n",
    "metrics = pd.DataFrame(metrics)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quarta comparação: o sample de dados interfere na capacidade do modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor</th>\n",
       "      <th>sample01</th>\n",
       "      <th>sample02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scores-cut8-basic/1</td>\n",
       "      <td>0.703752</td>\n",
       "      <td>0.707439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scores-cut6-full/1</td>\n",
       "      <td>0.709564</td>\n",
       "      <td>0.704225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scores-cut6-full/2</td>\n",
       "      <td>0.700698</td>\n",
       "      <td>0.706176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scores-cut8-full/1</td>\n",
       "      <td>0.708675</td>\n",
       "      <td>0.707936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scores-cut8-full/2</td>\n",
       "      <td>0.703235</td>\n",
       "      <td>0.705540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>scores-cut7-full/1</td>\n",
       "      <td>0.704263</td>\n",
       "      <td>0.704651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>scores-cut7-basic/2</td>\n",
       "      <td>0.700951</td>\n",
       "      <td>0.705210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>scores-cut7-full/2</td>\n",
       "      <td>0.705855</td>\n",
       "      <td>0.709543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>scores-cut8-basic/2</td>\n",
       "      <td>0.707420</td>\n",
       "      <td>0.707960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>scores-cut7-basic/1</td>\n",
       "      <td>0.706680</td>\n",
       "      <td>0.706703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>scores-cut6-basic/2</td>\n",
       "      <td>0.707292</td>\n",
       "      <td>0.702501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>scores-cut6-basic/1</td>\n",
       "      <td>0.703058</td>\n",
       "      <td>0.707050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 factor  sample01  sample02\n",
       "0   scores-cut8-basic/1  0.703752  0.707439\n",
       "1    scores-cut6-full/1  0.709564  0.704225\n",
       "2    scores-cut6-full/2  0.700698  0.706176\n",
       "3    scores-cut8-full/1  0.708675  0.707936\n",
       "4    scores-cut8-full/2  0.703235  0.705540\n",
       "5    scores-cut7-full/1  0.704263  0.704651\n",
       "6   scores-cut7-basic/2  0.700951  0.705210\n",
       "7    scores-cut7-full/2  0.705855  0.709543\n",
       "8   scores-cut8-basic/2  0.707420  0.707960\n",
       "9   scores-cut7-basic/1  0.706680  0.706703\n",
       "10  scores-cut6-basic/2  0.707292  0.702501\n",
       "11  scores-cut6-basic/1  0.703058  0.707050"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descreve fatores de análise\n",
    "factors: Set[str] = set()\n",
    "for dataset_type in df[\"dataset_type\"].unique():\n",
    "    for arch in df[\"arch\"].unique():\n",
    "        factors.add(f\"{dataset_type}/{arch}\")\n",
    "\n",
    "# Calcula as médias para cada fator\n",
    "metrics = []\n",
    "for factor in factors:\n",
    "    dt, arch = factor.split(\"/\")\n",
    "    sampe01, sample02 = 0.1, 0.2\n",
    "\n",
    "    metric = {\n",
    "        \"factor\": factor,\n",
    "        \"sample01\": [],\n",
    "        \"sample02\": []\n",
    "    }\n",
    "\n",
    "    subset = df.loc[\n",
    "        (df[\"dataset_type\"] == dt)\n",
    "        & (df[\"arch\"] == int(arch))\n",
    "    ]\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        with open(f\"{RESULTS_DIR}/{row[\"predict_file\"]}\") as file:\n",
    "            result = json.load(file)\n",
    "            \n",
    "        if row[\"sample\"] == sampe01:\n",
    "            metric[\"sample01\"].append(result[\"metrics\"][\"f1-score\"])\n",
    "\n",
    "        if row[\"sample\"] == sample02:\n",
    "            metric[\"sample02\"].append(result[\"metrics\"][\"f1-score\"])\n",
    "\n",
    "    metric[\"sample01\"] = np.mean(metric[\"sample01\"])\n",
    "    metric[\"sample02\"] = np.mean(metric[\"sample02\"])\n",
    "\n",
    "    metrics.append(metric)\n",
    "\n",
    "metrics = pd.DataFrame(metrics)\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
