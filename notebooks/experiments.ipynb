{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from datetime import date\n",
    "from typing import List, Set\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "DATE_LIMIT = date(2023, 12, 31)\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "USER_DATA_READ=f\"{BASE_PATH}/data/users-details-2023.csv\"\n",
    "USER_DATA_SAVE=f\"{BASE_PATH}/data/users.parquet\"\n",
    "\n",
    "ANIME_DATA_READ = f\"{BASE_PATH}/data/anime-dataset-2023.csv\"\n",
    "ANIME_DATA_SAVE = f\"{BASE_PATH}/data/animes.parquet\"\n",
    "\n",
    "SCORE_DATA_READ = f\"{BASE_PATH}/data/users-score-2023.csv\"\n",
    "SCORE_DATA_SAVE = f\"{BASE_PATH}/data/scores.parquet\"\n",
    "\n",
    "FINAL_DATASET_CUT6_BASIC_USER_DATA = f\"{BASE_PATH}/data/scores-cut6-basic.parquet\"\n",
    "FINAL_DATASET_CUT7_BASIC_USER_DATA = f\"{BASE_PATH}/data/scores-cut7-basic.parquet\"\n",
    "FINAL_DATASET_CUT8_BASIC_USER_DATA = f\"{BASE_PATH}/data/scores-cut8-basic.parquet\"\n",
    "\n",
    "FINAL_DATASET_CUT6_FULL_USER_DATA = f\"{BASE_PATH}/data/scores-cut6-full.parquet\"\n",
    "FINAL_DATASET_CUT7_FULL_USER_DATA = f\"{BASE_PATH}/data/scores-cut7-full.parquet\"\n",
    "FINAL_DATASET_CUT8_FULL_USER_DATA = f\"{BASE_PATH}/data/scores-cut8-full.parquet\"\n",
    "\n",
    "EXPERIMENT_LOG = f\"{BASE_PATH}/data/experiment-log.txt\"\n",
    "RESULTS_DIR = f\"{BASE_PATH}/data/results\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseReader:\n",
    "    def __init__(self, read_path: str, save_path: str):\n",
    "        self.file_path = read_path\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def to_parquet(self, df: pd.DataFrame) -> None:\n",
    "        df.to_parquet(self.save_path, index=False)\n",
    "\n",
    "    def get_stats(self, df: pd.DataFrame, columns: List[str]) -> dict:\n",
    "        result = dict()\n",
    "        for c in columns:\n",
    "            result[c] = {\n",
    "                \"hist\": df[c].value_counts(dropna=False).to_dict(),\n",
    "                \"max\": df[c].max(skipna=True) if df[c].dtype != \"O\" else 0,\n",
    "                \"mean\": df[c].mean(skipna=True) if df[c].dtype != \"O\" else 0,\n",
    "                \"median\": df[c].median(skipna=True) if df[c].dtype != \"O\" else 0,\n",
    "                \"min\": df[c].min(skipna=True) if df[c].dtype != \"O\" else 0\n",
    "            }\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def show_stats(self, result: dict) -> None:\n",
    "        for column in result.keys():\n",
    "            # Exibe estatísticas descritivas básicas\n",
    "            print(f\"Estatística descritiva de \\\"{column}\\\"\")\n",
    "            print(f\"Mínimo: {result[column][\"min\"]}\")\n",
    "            print(f\"Média: {result[column][\"mean\"]}\")\n",
    "            print(f\"Mediana: {result[column][\"median\"]}\")\n",
    "            print(f\"Máximo: {result[column][\"max\"]}\")\n",
    "\n",
    "            # Avalia a quantidade de nulos\n",
    "            count = 0\n",
    "            null = 0\n",
    "            for k in result[column][\"hist\"].keys():\n",
    "                count = count + result[column][\"hist\"][k]\n",
    "                if type(k) == float and np.isnan(k):\n",
    "                    null = result[column][\"hist\"][k]\n",
    "            percent = round(null * 100 / count, 2) if count > 0 else 0\n",
    "            print(f\"Quantidade de nulos: {null} ({percent}%)\")\n",
    "\n",
    "            # Exibe uma linha de separação\n",
    "            print(\"*\" * 40, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserReader(BaseReader):\n",
    "    def __init__(self, read_path: str, save_path: str):\n",
    "        super().__init__(read_path, save_path)\n",
    "\n",
    "    def first_process(self) -> pd.DataFrame:\n",
    "        # Carrega os dados, removendo colunas não utilizadas\n",
    "        remove_columns = [\n",
    "            \"Username\", \"Location\", \"Joined\",\n",
    "            \"On Hold\", \"Plan to Watch\", \"Rewatched\"\n",
    "        ]\n",
    "        df = pd.read_csv(self.file_path).drop(remove_columns, axis=1)\n",
    "\n",
    "        # Faz a troca de gênero definindo Male = 0 e Female = 1\n",
    "        def clear_gender(value: str) -> int:\n",
    "            if type(value) != str:\n",
    "                return None\n",
    "            return 0 if value.upper() == \"MALE\" else 1\n",
    "        df[\"Gender\"] = df[\"Gender\"].apply(clear_gender)\n",
    "\n",
    "        # Faz a conversão da data de nascimento na idade\n",
    "        def get_age(birth_date: str | float):\n",
    "            if type(birth_date) != str:\n",
    "                return None\n",
    "            return int((DATE_LIMIT - date.fromisoformat(birth_date.split(\"T\")[0])).days / 365)\n",
    "        df[\"age\"] = df[\"Birthday\"].apply(get_age)\n",
    "        df = df.drop([\"Birthday\"], axis=1)\n",
    "\n",
    "        # Faz a troca de nomes de colunas\n",
    "        df = df.rename(columns={\n",
    "            \"Mal ID\": \"user_id\",\n",
    "            \"Gender\": \"gender\",\n",
    "            \"Days Watched\": \"days_spent_with_anime\",\n",
    "            \"Mean Score\": \"mean_score\",\n",
    "            \"Watching\": \"current_anime_wathing\",\n",
    "            \"Completed\": \"total_anime_watched\",\n",
    "            \"Dropped\": \"dropped_anime\",\n",
    "            \"Total Entries\": \"anime_in_list\",\n",
    "            \"Episodes Watched\": \"episodes_watched\"\n",
    "        })\n",
    "\n",
    "        # Salva o arquivo limpo\n",
    "        return df\n",
    "\n",
    "    def remove_nulls(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        original_rows = len(df)\n",
    "        df = df.dropna()\n",
    "        new_rows = len(df)\n",
    "        percent = round((original_rows - new_rows) * 100 / original_rows, 2)\n",
    "        print(f\"Remoção de {original_rows - new_rows} linhas ({percent}%)\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_user_analysis():\n",
    "    reader = UserReader(USER_DATA_READ, USER_DATA_SAVE)\n",
    "    df_user = reader.first_process()\n",
    "\n",
    "    stats = reader.get_stats(\n",
    "        df_user,\n",
    "        [\n",
    "            \"gender\", \"days_spent_with_anime\", \"mean_score\",\n",
    "            \"current_anime_wathing\", \"total_anime_watched\",\n",
    "            \"dropped_anime\", \"anime_in_list\", \"episodes_watched\", \"age\"\n",
    "        ]\n",
    "    )\n",
    "    reader.show_stats(stats)\n",
    "\n",
    "    df_user = reader.remove_nulls(df_user)\n",
    "    reader.to_parquet(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_user_analysis()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeReader(BaseReader):\n",
    "    def __init__(self, read_path: str, save_path: str):\n",
    "        super().__init__(read_path, save_path)\n",
    "\n",
    "    def first_process(self) -> pd.DataFrame:\n",
    "        # Carrega dados\n",
    "        df = pd.read_csv(self.file_path)\n",
    "\n",
    "        # Remove colunas não utilizadas\n",
    "        use_columns = [\"anime_id\", \"Genres\", \"Episodes\", \"Source\", \"Duration\"]\n",
    "        df = df[use_columns]\n",
    "\n",
    "        # Faz a conversão do texto de duração para o valor numérico\n",
    "        def extract_duration(description: str):\n",
    "            if description.upper() == \"UNKNOWN\":\n",
    "                return np.nan\n",
    "            numbers = re.findall(r\"[0-9]+\", description)\n",
    "            if len(numbers) == 2:\n",
    "                return int(numbers[0]) * 60 + int(numbers[1])\n",
    "            else:\n",
    "                return int(numbers[0])\n",
    "        df[\"Duration\"] = df[\"Duration\"].apply(extract_duration)\n",
    "\n",
    "        # Converte o número de episódios em números e remove nulos\n",
    "        df[\"Episodes\"] = df[\"Episodes\"].apply(lambda x: float(x) if x.upper() != \"UNKNOWN\" else np.nan).astype(\"float64\")\n",
    "\n",
    "        # Aplica uma padronização nos nomes dos materiais originais\n",
    "        def standard_source(source: str):\n",
    "            conv_source = {\n",
    "                \"4-koma manga\": \"manga\",\n",
    "                \"Book\": \"book\",\n",
    "                \"Card game\": \"game\",\n",
    "                \"Game\": \"game\",\n",
    "                \"Light novel\": \"novel\",\n",
    "                \"Manga\": \"manga\",\n",
    "                \"Mixed media\": \"other\",\n",
    "                \"Music\": \"other\",\n",
    "                \"Novel\": \"novel\",\n",
    "                \"Original\": \"original\",\n",
    "                \"Other\": \"other\",\n",
    "                \"Picture book\": \"other\",\n",
    "                \"Radio\": \"other\",\n",
    "                \"Unknown\": np.nan,\n",
    "                \"Visual novel\": \"visual_novel\",\n",
    "                \"Web manga\": \"manga\",\n",
    "                \"Web novel\": \"novel\"\n",
    "            }\n",
    "            try:\n",
    "                return conv_source[source]\n",
    "            except:\n",
    "                return np.nan\n",
    "        df[\"Source\"] = df[\"Source\"].apply(standard_source)\n",
    "\n",
    "        # Resolve nomenclatura de gêneros\n",
    "        df[\"Genres\"] = df[\"Genres\"].apply(lambda x: np.nan if x == \"UNKNOWN\" else x)\n",
    "\n",
    "        # Faz a troca dos nomes das colunas\n",
    "        df = df.rename(columns={\n",
    "            \"Genres\": \"genres\",\n",
    "            \"Episodes\": \"episodes\",\n",
    "            \"Source\": \"source\",\n",
    "            \"Duration\": \"duration\"\n",
    "        })\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def remove_nulls(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.dropna(subset=[\"source\", \"duration\", \"episodes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_anime_analysis():\n",
    "    anime_reader = AnimeReader(ANIME_DATA_READ, ANIME_DATA_SAVE)\n",
    "\n",
    "    df_anime = anime_reader.first_process()\n",
    "    stats = anime_reader.get_stats(\n",
    "        df_anime,\n",
    "        [\"genres\", \"episodes\", \"source\", \"duration\"]\n",
    "    )\n",
    "    anime_reader.show_stats(stats)\n",
    "    df_anime = anime_reader.remove_nulls(df_anime)\n",
    "    anime_reader.to_parquet(df_anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_anime_analysis()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreReader(BaseReader):\n",
    "    def __init__(self, read_path: str, save_path: str, user_path: str, anime_path: str):\n",
    "        super().__init__(read_path, save_path)\n",
    "        self.anime_path = anime_path\n",
    "        self.user_path = user_path\n",
    "\n",
    "    def make_dataset(self, rating_cut=7, user_merge_mode=1) -> pd.DataFrame:\n",
    "        # Verifica integridade dos parâmetros\n",
    "        if rating_cut > 10 or rating_cut < 1:\n",
    "            raise Exception(\"O corte da classificação deve ser entre 1 e 10\")\n",
    "        \n",
    "        if user_merge_mode not in [1, 2]:\n",
    "            raise Exception(\"O modo de merge de usuário deve ser 1 ou 2\")\n",
    "        \n",
    "        # Carrega os dados dos scores, limpando as colunas não utilizadas\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        df = df.drop([\"Username\", \"Anime Title\"], axis=1)\n",
    "\n",
    "        # Carrega os dados de usuários e animes\n",
    "        users = pd.read_parquet(self.user_path)\n",
    "        animes = pd.read_parquet(self.anime_path)\n",
    "\n",
    "        # Recupera todos os gêneros possíveis\n",
    "        genres = [[s.strip() for s in g.split(\",\")] for g in animes[\"genres\"].values if g is not None]\n",
    "        genres: Set[str] = set(itertools.chain.from_iterable(genres))\n",
    "\n",
    "        # Define a função de verificação de gênero\n",
    "        # Os dados de gêneros são carregados como uma string,\n",
    "        # com as categorias separadas por vírgula\n",
    "        def verify_genre(genres: str | None, genre: str) -> int:\n",
    "            if genres is None:\n",
    "                return 0\n",
    "            \n",
    "            genres = [s.lower().strip() for s in genres.split(\",\")]\n",
    "            return 1 if genre.lower() in genres else 0\n",
    "\n",
    "        # Aplica o encoder para gêneros de animes\n",
    "        for genre in genres:\n",
    "            column = f\"genre_{\"_\".join(genre.lower().split(\" \"))}\"\n",
    "            animes[column] = animes[\"genres\"].apply(lambda x: verify_genre(x, genre))\n",
    "        animes = animes.drop([\"genres\"], axis=1)\n",
    "\n",
    "        # Define um encoder para o material original do anime\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "        encoder.fit(animes[[\"source\"]])\n",
    "\n",
    "        # Atualiza os dados de anime com o encoder de material original\n",
    "        encoder_df = pd.DataFrame(\n",
    "            encoder.transform(animes[[\"source\"]]),\n",
    "            columns=encoder.get_feature_names_out()\n",
    "        )\n",
    "        animes = pd.concat((animes, encoder_df), axis=1)\n",
    "        animes = animes.drop([\"source\"], axis=1)\n",
    "\n",
    "        # Executa o merge com os dados de usuários\n",
    "        # user_merge_mode = 1 faz com que apenas os dados básicos sejam usados\n",
    "        # user_merge_mode = 2 utiliza todos os dados de usuários\n",
    "        if user_merge_mode == 1:\n",
    "            users = users[[\"user_id\", \"gender\", \"age\"]]\n",
    "\n",
    "        if user_merge_mode == 2:\n",
    "            users = users[[\"user_id\", \"gender\", \"age\", \"days_spent_with_anime\", \"total_anime_watched\", \"dropped_anime\", \"mean_score\"]]\n",
    "        \n",
    "        df = df.merge(users, how=\"inner\", on=\"user_id\")\n",
    "\n",
    "        # Executa o merge com os dados de animes\n",
    "        df = df.merge(animes, how=\"inner\", on=\"anime_id\")\n",
    "\n",
    "        # Faz a criação da coluna target\n",
    "        df[\"target\"] = df[\"rating\"].apply(lambda x: 1 if x > rating_cut else 0)\n",
    "        df = df.drop([\"rating\"], axis=1)\n",
    "\n",
    "        # Finaliza o processo, removendo colunas de ID\n",
    "        df = df.drop([\"user_id\", \"anime_id\"], axis=1)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets():\n",
    "    result_files = [\n",
    "        FINAL_DATASET_CUT6_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT7_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT8_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT6_FULL_USER_DATA,\n",
    "        FINAL_DATASET_CUT7_FULL_USER_DATA,\n",
    "        FINAL_DATASET_CUT8_FULL_USER_DATA\n",
    "    ]\n",
    "\n",
    "    for save_path in result_files:\n",
    "        score_reader = ScoreReader(\n",
    "            SCORE_DATA_READ,\n",
    "            save_path,\n",
    "            USER_DATA_SAVE,\n",
    "            ANIME_DATA_SAVE\n",
    "        )\n",
    "        scores = score_reader.make_dataset()\n",
    "        score_reader.to_parquet(scores)\n",
    "\n",
    "        del scores\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_datasets()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4Layers(nn.Module):\n",
    "    def __init__(self, n_features: int, n_classes=2, n_neurons=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_neurons)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc4 = nn.Linear(n_neurons, n_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.out = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Model8Layers(nn.Module):\n",
    "    def __init__(self, n_features: int, n_classes=2, n_neurons=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_neurons)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc4 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc5 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc6 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc7 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc8 = nn.Linear(n_neurons, n_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.out = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manager:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_dataset(self, read_path: str, sample=0.2):\n",
    "        # Carrega dados processados\n",
    "        df = pd.read_parquet(read_path).sample(frac=sample)\n",
    "        X = df.drop([\"target\"], axis=1).values\n",
    "        y = df[\"target\"].values\n",
    "\n",
    "        # Faz a divisão entre treino e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        # Aplica a padronização de valores\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Faz a transformação de numpy array para tensor\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        # Instancia dataset de tensores\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "        # Instancia loader de tensores\n",
    "        train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "        return train_dataset, test_dataset, train_loader, test_loader\n",
    "    \n",
    "    def execute(self, train_dataset: DataLoader, train_loader: DataLoader, epochs=100, n_neurons=16, arch=1):\n",
    "        # Garante consistência da arquitetura\n",
    "        if arch not in [1, 2]:\n",
    "            raise Exception(\"As arquiteturas válidas são 1 e 2\")\n",
    "        \n",
    "        # Regitra o tempo de início do treinamento\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Carrega dados e inicializa modelo\n",
    "        classes_ = len(train_dataset.tensors[1].unique())\n",
    "        if arch == 1:\n",
    "            model = Model4Layers(\n",
    "                n_features=train_dataset.tensors[0].shape[1],\n",
    "                n_classes=classes_,\n",
    "                n_neurons=n_neurons\n",
    "            )\n",
    "        elif arch == 2:\n",
    "            model = Model8Layers(\n",
    "                n_features=train_dataset.tensors[0].shape[1],\n",
    "                n_classes=classes_,\n",
    "                n_neurons=n_neurons\n",
    "            )\n",
    "\n",
    "        # Define o modo de otimização\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(0, epochs):\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            model.train()\n",
    "            count_batch = 0\n",
    "            limit_batch = (train_dataset.tensors[0].shape[0] // train_loader.batch_size) + 1\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                percent = round(count_batch * 100 / limit_batch, 2)\n",
    "                print(f\"Epoch {epoch + 1} Batch {count_batch + 1} ({percent}%)\", end=\"\\r\")\n",
    "                inputs = inputs\n",
    "                labels = labels\n",
    "\n",
    "                # Inicia os gradientes e calcula a predição\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                pred_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                # Teoricamente, seria preciso passar as labels do dataset para o\n",
    "                # padrão one hot encoder, porém a camada softmax no modelo já\n",
    "                # resolve isso.\n",
    "                # oh_labels = F.one_hot(labels.long())\n",
    "                # loss = criterion(outputs, torch.reshape(oh_labels, (oh_labels.size()[0], classes_)).float())\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Calcula os gradientes e atualiza os pesos\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Fal a atualização das estatísticas de acompanhamento\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(pred_labels == labels.data).item()\n",
    "                count_batch = count_batch + 1\n",
    "\n",
    "            # Exibe estatísticas de acompanhamento\n",
    "            num_samples = len(train_dataset)\n",
    "            epoch_loss = running_loss / num_samples\n",
    "            epoch_accuracy = running_corrects / num_samples\n",
    "            print(f\"Epoch {epoch + 1}: Loss {epoch_loss:.3f} Acurácia {epoch_accuracy:.3f}\")\n",
    "\n",
    "        # Calcula o tempo de execução do treinamento\n",
    "        end_time = time.time()\n",
    "        train_time = end_time - start_time\n",
    "\n",
    "        return model, train_time, epoch_loss\n",
    "    \n",
    "    def compute_test(self, model: nn.Module, test_loader: DataLoader, train_time: float, train_loss: float) -> dict:\n",
    "        # Define modelo como avaliação e inicia as listas de labels\n",
    "        model.eval()\n",
    "        pred_labels_all = []\n",
    "        true_labels_all = []\n",
    "\n",
    "        # Passa pelo loader para cálculo das predições\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            pred_labels = torch.argmax(outputs, dim=1)\n",
    "            pred_labels_all.append(pred_labels)\n",
    "            true_labels_all.append(labels)\n",
    "\n",
    "        # Concatena os resultados\n",
    "        pred_labels = torch.cat(pred_labels_all, dim=0).cpu().numpy()\n",
    "        true_labels = torch.cat(true_labels_all, dim=0).numpy()\n",
    "\n",
    "        # Registra dados no dicionário de dados\n",
    "        return {\n",
    "            \"train_time\": train_time,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"metrics\": {\n",
    "                \"accuracy\": (pred_labels == true_labels).mean(),\n",
    "                \"f1-score\": f1_score(true_labels, pred_labels, pos_label=1, average=\"binary\"),\n",
    "                \"recall-score\": recall_score(true_labels, pred_labels, pos_label=1, average=\"binary\"),\n",
    "                \"precission-score\": precision_score(true_labels, pred_labels, pos_label=1, average=\"binary\")\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_experiments():\n",
    "    # Define os parâmetros dos experimentos\n",
    "    archs_set = [1, 2]\n",
    "    neurons_set = [16]\n",
    "    sample_data = [0.1, 0.2]\n",
    "    epochs = [10]\n",
    "    repeat = 5\n",
    "    data_paths = [\n",
    "        FINAL_DATASET_CUT6_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT7_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT8_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT6_FULL_USER_DATA,\n",
    "        FINAL_DATASET_CUT7_FULL_USER_DATA,\n",
    "        FINAL_DATASET_CUT8_FULL_USER_DATA\n",
    "    ]\n",
    "\n",
    "    # Verifica o log de experimentos\n",
    "    if not os.path.exists(EXPERIMENT_LOG):\n",
    "        with open(EXPERIMENT_LOG, \"w\") as file:\n",
    "            file.write(\"dataset_type,arch,neurons,sample,epochs,iteration,weight_file,predict_file\\n\")\n",
    "\n",
    "    # Função auxiliar: abre o log e verifica registros\n",
    "    def verify(dataset_type: str, arch: int, neurons: int, sample: float, epochs: int, iteration: int):\n",
    "        exist = False\n",
    "        with open(EXPERIMENT_LOG, \"r\") as file:\n",
    "            row = file.readline()\n",
    "            while row:\n",
    "                row_dataset_type, row_arch, row_neurons, row_sample, row_epochs, row_iteration, _, _ = row.split(\",\")\n",
    "                row_params = [row_dataset_type, row_arch, row_neurons, row_sample, row_epochs, row_iteration]\n",
    "                search_params = [str(dataset_type), str(arch), str(neurons), str(sample), str(epochs), str(iteration)]\n",
    "                #print(row_params, search_params)\n",
    "                \n",
    "                if row_params == search_params:\n",
    "                    exist = True\n",
    "                    break\n",
    "\n",
    "                row = file.readline()\n",
    "\n",
    "        return exist\n",
    "\n",
    "    for data_path in data_paths:\n",
    "        # Tipo de dataset utilizado\n",
    "        dataset_type = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        for neurons in neurons_set:\n",
    "            # Quantidade de neurônios das camadas internas\n",
    "\n",
    "            for sample in sample_data:\n",
    "                # Porção dos dados fracionados\n",
    "\n",
    "                for epoch in epochs:\n",
    "                    # Quantidade de épocas do treinamento\n",
    "\n",
    "                    for arch in archs_set:\n",
    "                        # Profundidade da rede\n",
    "\n",
    "                        for i in range(0, repeat):\n",
    "                            # Verifica se o experimento já foi executado\n",
    "                            if verify(dataset_type, arch, neurons, sample, epoch, i):\n",
    "                                continue\n",
    "\n",
    "                            # Registra todos os dados do experimento\n",
    "                            unique_name = int(time.time())\n",
    "                            weight_path = f\"{RESULTS_DIR}/{unique_name}.pth\"\n",
    "                            predict_path = f\"{RESULTS_DIR}/{unique_name}.json\"\n",
    "                            experiment_data = [\n",
    "                                    dataset_type,\n",
    "                                    str(arch),\n",
    "                                    str(neurons),\n",
    "                                    str(sample),\n",
    "                                    str(epoch),\n",
    "                                    str(i),\n",
    "                                    f\"{unique_name}.pth\",\n",
    "                                    f\"{unique_name}.json\"\n",
    "                                ]\n",
    "\n",
    "                            # Log de execução\n",
    "                            print(f\"Execução do experimento {\",\".join(experiment_data[:-2])}\".upper())\n",
    "\n",
    "                            # Repetição do experimento\n",
    "                            process = Manager()\n",
    "                            train_dataset, test_dataset, train_loader, test_loader = process.get_dataset(data_path, sample=sample)\n",
    "                            gc.collect()\n",
    "                            model, train_time, train_loss = process.execute(train_dataset, train_loader, epochs=epoch, n_neurons=neurons, arch=arch)\n",
    "\n",
    "                            # Executa o teste do modelo\n",
    "                            results = process.compute_test(model, test_loader, train_time, train_loss)\n",
    "                            \n",
    "                            # Salva o json de métricas\n",
    "                            with open(predict_path, \"w+\") as file:\n",
    "                                file.write(json.dumps(results))\n",
    "\n",
    "                            # Salva os pesos do modelo\n",
    "                            torch.save(model.state_dict(), weight_path)\n",
    "\n",
    "                            # Registra no log\n",
    "                            with open(EXPERIMENT_LOG, \"a\") as file:\n",
    "                                file.write(\",\".join(experiment_data) + \"\\n\")\n",
    "\n",
    "                            # Libera memória\n",
    "                            del train_dataset, test_dataset, train_loader, test_loader\n",
    "                            gc.collect()\n",
    "                            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
