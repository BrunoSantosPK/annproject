{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definições\n",
    "\n",
    "Todas as bibliotecas utilizadas durante as execuções são carregadas aqui. Como atenção, o módulo de funções do Pytorch é importado apenas para deixar um exemplo de uso do one hot encoder do Pytorch. Tal processamento foi suprimido na versão final, porém o código foi mantido comentado para questão de consulta.\n",
    "\n",
    "O ponto crítico desta etapa é a definição correta dos diretórios de carregamento de arquivos de dados, bem como os caminhos onde serão salvos os dados processados para processamento durante o treinamento e análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "from datetime import date\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Set, Tuple\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "DATE_LIMIT = date(2023, 12, 31)\n",
    "BASE_PATH = os.path.dirname(os.getcwd())\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "USER_DATA_READ=f\"{BASE_PATH}/data/users-details-2023.csv\"\n",
    "USER_DATA_SAVE=f\"{BASE_PATH}/data/users.parquet\"\n",
    "\n",
    "ANIME_DATA_READ = f\"{BASE_PATH}/data/anime-dataset-2023.csv\"\n",
    "ANIME_DATA_SAVE = f\"{BASE_PATH}/data/animes.parquet\"\n",
    "\n",
    "SCORE_DATA_READ = f\"{BASE_PATH}/data/users-score-2023.csv\"\n",
    "SCORE_DATA_SAVE = f\"{BASE_PATH}/data/scores.parquet\"\n",
    "\n",
    "FINAL_DATASET_CUT6_BASIC_USER_DATA = f\"{BASE_PATH}/data/scores-cut6-basic.parquet\"\n",
    "FINAL_DATASET_CUT7_BASIC_USER_DATA = f\"{BASE_PATH}/data/scores-cut7-basic.parquet\"\n",
    "FINAL_DATASET_CUT8_BASIC_USER_DATA = f\"{BASE_PATH}/data/scores-cut8-basic.parquet\"\n",
    "\n",
    "FINAL_DATASET_CUT6_FULL_USER_DATA = f\"{BASE_PATH}/data/scores-cut6-full.parquet\"\n",
    "FINAL_DATASET_CUT7_FULL_USER_DATA = f\"{BASE_PATH}/data/scores-cut7-full.parquet\"\n",
    "FINAL_DATASET_CUT8_FULL_USER_DATA = f\"{BASE_PATH}/data/scores-cut8-full.parquet\"\n",
    "\n",
    "EXPERIMENT_LOG = f\"{BASE_PATH}/data/experiment-log.txt\"\n",
    "RESULTS_DIR = f\"{BASE_PATH}/data/results\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De modo genérico, o fluxo de processamento de dados contará sempre com uma forma de calcular e exibir informações de estatística descritiva e salvar resultado em arquivos .parquet. Esta super classe de leitura se encarrega de implementar estas funcionalidades básicas e genéricas. O cálculo da estatística descritiva já está programado para lidar de forma diferente com variáveis categóricas e numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseReader:\n",
    "    def __init__(self, read_path: str, save_path: str):\n",
    "        self.file_path = read_path\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def to_parquet(self, df: pd.DataFrame) -> None:\n",
    "        df.to_parquet(self.save_path, index=False)\n",
    "\n",
    "    def get_stats(self, df: pd.DataFrame, columns: List[str]) -> dict:\n",
    "        result = dict()\n",
    "        for c in columns:\n",
    "            result[c] = {\n",
    "                \"hist\": df[c].value_counts(dropna=False).to_dict(),\n",
    "                \"max\": df[c].max(skipna=True) if df[c].dtype != \"O\" else 0,\n",
    "                \"mean\": df[c].mean(skipna=True) if df[c].dtype != \"O\" else 0,\n",
    "                \"median\": df[c].median(skipna=True) if df[c].dtype != \"O\" else 0,\n",
    "                \"min\": df[c].min(skipna=True) if df[c].dtype != \"O\" else 0\n",
    "            }\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def show_stats(self, result: dict) -> None:\n",
    "        for column in result.keys():\n",
    "            # Exibe estatísticas descritivas básicas\n",
    "            print(f\"Estatística descritiva de \\\"{column}\\\"\")\n",
    "            print(f\"Mínimo: {result[column][\"min\"]}\")\n",
    "            print(f\"Média: {result[column][\"mean\"]}\")\n",
    "            print(f\"Mediana: {result[column][\"median\"]}\")\n",
    "            print(f\"Máximo: {result[column][\"max\"]}\")\n",
    "\n",
    "            # Avalia a quantidade de nulos\n",
    "            count = 0\n",
    "            null = 0\n",
    "            for k in result[column][\"hist\"].keys():\n",
    "                count = count + result[column][\"hist\"][k]\n",
    "                if type(k) == float and np.isnan(k):\n",
    "                    null = result[column][\"hist\"][k]\n",
    "            percent = round(null * 100 / count, 2) if count > 0 else 0\n",
    "            print(f\"Quantidade de nulos: {null} ({percent}%)\")\n",
    "\n",
    "            # Exibe uma linha de separação\n",
    "            print(\"*\" * 40, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A leitura e processamento da base de usuários define as colunas que serão utilizadas no treinamento, aplicando as seguintes regras:\n",
    "\n",
    "- Padronização de gênero, atribuindo 0 para MALE e 1 para FEMALE;\n",
    "- Cálculo da idade a partir da data de nascimento, considerando a data de geração dos dados;\n",
    "- Padronização dos nomes das colunas, removendo espaços em branco e letras maiúsculas;\n",
    "- Remoção de linhas com valores nulos para gênero e idade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserReader(BaseReader):\n",
    "    def __init__(self, read_path: str, save_path: str):\n",
    "        super().__init__(read_path, save_path)\n",
    "\n",
    "    def first_process(self) -> pd.DataFrame:\n",
    "        # Carrega os dados, removendo colunas não utilizadas\n",
    "        remove_columns = [\n",
    "            \"Username\", \"Location\", \"Joined\",\n",
    "            \"On Hold\", \"Plan to Watch\", \"Rewatched\"\n",
    "        ]\n",
    "        df = pd.read_csv(self.file_path).drop(remove_columns, axis=1)\n",
    "\n",
    "        # Faz a troca de gênero definindo Male = 0 e Female = 1\n",
    "        def clear_gender(value: str) -> int:\n",
    "            if type(value) != str:\n",
    "                return None\n",
    "            return 0 if value.upper() == \"MALE\" else 1\n",
    "        df[\"Gender\"] = df[\"Gender\"].apply(clear_gender)\n",
    "\n",
    "        # Faz a conversão da data de nascimento na idade\n",
    "        def get_age(birth_date: str | float):\n",
    "            if type(birth_date) != str:\n",
    "                return None\n",
    "            return int((DATE_LIMIT - date.fromisoformat(birth_date.split(\"T\")[0])).days / 365)\n",
    "        df[\"age\"] = df[\"Birthday\"].apply(get_age)\n",
    "        df = df.drop([\"Birthday\"], axis=1)\n",
    "\n",
    "        # Faz a troca de nomes de colunas\n",
    "        df = df.rename(columns={\n",
    "            \"Mal ID\": \"user_id\",\n",
    "            \"Gender\": \"gender\",\n",
    "            \"Days Watched\": \"days_spent_with_anime\",\n",
    "            \"Mean Score\": \"mean_score\",\n",
    "            \"Watching\": \"current_anime_wathing\",\n",
    "            \"Completed\": \"total_anime_watched\",\n",
    "            \"Dropped\": \"dropped_anime\",\n",
    "            \"Total Entries\": \"anime_in_list\",\n",
    "            \"Episodes Watched\": \"episodes_watched\"\n",
    "        })\n",
    "\n",
    "        # Salva o arquivo limpo\n",
    "        return df\n",
    "\n",
    "    def remove_nulls(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        original_rows = len(df)\n",
    "        df = df.dropna()\n",
    "        new_rows = len(df)\n",
    "        percent = round((original_rows - new_rows) * 100 / original_rows, 2)\n",
    "        print(f\"Remoção de {original_rows - new_rows} linhas ({percent}%)\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo o processo de treinamento é encapsulado na seguinte função. As etapas compreendem o carregamento de dados, a aplicação das regras de limpeza descritas na classe de leitura, exibição das estatísticas descritivas das colunas, remoção de nulos e gravação de dados em parquet.\n",
    "\n",
    "Importante comentar que a remoção de nulos foi implementada após a análise da estatística descritiva dos dados, bem como o ajuste das lógicas de processamento de dados da classe de leitura.\n",
    "\n",
    "OBS.: Todo o processo é encapsulado dentro de funções para evitar consumo exagerado de memória, garantido também pela chamada explícita do garbage collector ao final da função para evitar lixo na memória durante a execução deste notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_user_analysis():\n",
    "    reader = UserReader(USER_DATA_READ, USER_DATA_SAVE)\n",
    "    df_user = reader.first_process()\n",
    "\n",
    "    stats = reader.get_stats(\n",
    "        df_user,\n",
    "        [\n",
    "            \"gender\", \"days_spent_with_anime\", \"mean_score\",\n",
    "            \"current_anime_wathing\", \"total_anime_watched\",\n",
    "            \"dropped_anime\", \"anime_in_list\", \"episodes_watched\", \"age\"\n",
    "        ]\n",
    "    )\n",
    "    reader.show_stats(stats)\n",
    "\n",
    "    df_user = reader.remove_nulls(df_user)\n",
    "    reader.to_parquet(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_user_analysis()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De modo semelhante, a classe de leitura da base de dados de animes também herda da super classe de leitura. O processamento destes dados conta com uma padronização de categorias para o material original, removendo granularidade excessiva, já que, por exemplo, \"4-koma mangá\" é um tipo de organização de quadros de \"mangá\", porém isto não altera o caso de se tratar do mesmo tipo de material original, que é \"mangá\".\n",
    "\n",
    "A duração na base não é informada de forma numérica, mas como texto e com as indicações de \"horas\" e \"minutos\". Para recuperar apenas os dados numéricos é aplicada uma função regular para extrair os valores e salvá-los. Outro caso é que para casos em que o número de episódios não está disponível, o valor \"UNKNOWN\" é cadastrado. Isto gera a necessidade de aplicar uma regra para substituir este dado por NaN e converter o tipo de dados da coluna para float.\n",
    "\n",
    "A presença do valor \"UNKNOWN\" também ocorre na coluna de gêneros, sendo necessário a sua remoção. Porém, diferente de outros casos da substituição por NaN, a coluna gêneros é tratada diferente. Os gêneros são valores de multilabel, isso significa que um mesmo anime pode ter um ou mais gêneros. Para a entrada do modelo, os gêneros são pivotados e seu valores se tornam 1 para o caso do anime possuir o gênero ou 0 para o caso contrário.\n",
    "\n",
    "A remoção de nulos é feita apenas de material original, duração e quantidade de episódios. Também, todo o processamento aqui descrito foi feito a partir da visualização das estatísticas descritivas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeReader(BaseReader):\n",
    "    def __init__(self, read_path: str, save_path: str):\n",
    "        super().__init__(read_path, save_path)\n",
    "\n",
    "    def first_process(self) -> pd.DataFrame:\n",
    "        # Carrega dados\n",
    "        df = pd.read_csv(self.file_path)\n",
    "\n",
    "        # Remove colunas não utilizadas\n",
    "        use_columns = [\"anime_id\", \"Genres\", \"Episodes\", \"Source\", \"Duration\"]\n",
    "        df = df[use_columns]\n",
    "\n",
    "        # Faz a conversão do texto de duração para o valor numérico\n",
    "        def extract_duration(description: str):\n",
    "            if description.upper() == \"UNKNOWN\":\n",
    "                return np.nan\n",
    "            numbers = re.findall(r\"[0-9]+\", description)\n",
    "            if len(numbers) == 2:\n",
    "                return int(numbers[0]) * 60 + int(numbers[1])\n",
    "            else:\n",
    "                return int(numbers[0])\n",
    "        df[\"Duration\"] = df[\"Duration\"].apply(extract_duration)\n",
    "\n",
    "        # Converte o número de episódios em números e remove nulos\n",
    "        df[\"Episodes\"] = df[\"Episodes\"].apply(lambda x: float(x) if x.upper() != \"UNKNOWN\" else np.nan).astype(\"float64\")\n",
    "\n",
    "        # Aplica uma padronização nos nomes dos materiais originais\n",
    "        def standard_source(source: str):\n",
    "            conv_source = {\n",
    "                \"4-koma manga\": \"manga\",\n",
    "                \"Book\": \"book\",\n",
    "                \"Card game\": \"game\",\n",
    "                \"Game\": \"game\",\n",
    "                \"Light novel\": \"novel\",\n",
    "                \"Manga\": \"manga\",\n",
    "                \"Mixed media\": \"other\",\n",
    "                \"Music\": \"other\",\n",
    "                \"Novel\": \"novel\",\n",
    "                \"Original\": \"original\",\n",
    "                \"Other\": \"other\",\n",
    "                \"Picture book\": \"other\",\n",
    "                \"Radio\": \"other\",\n",
    "                \"Unknown\": np.nan,\n",
    "                \"Visual novel\": \"visual_novel\",\n",
    "                \"Web manga\": \"manga\",\n",
    "                \"Web novel\": \"novel\"\n",
    "            }\n",
    "            try:\n",
    "                return conv_source[source]\n",
    "            except:\n",
    "                return np.nan\n",
    "        df[\"Source\"] = df[\"Source\"].apply(standard_source)\n",
    "\n",
    "        # Resolve nomenclatura de gêneros\n",
    "        df[\"Genres\"] = df[\"Genres\"].apply(lambda x: np.nan if x == \"UNKNOWN\" else x)\n",
    "\n",
    "        # Faz a troca dos nomes das colunas\n",
    "        df = df.rename(columns={\n",
    "            \"Genres\": \"genres\",\n",
    "            \"Episodes\": \"episodes\",\n",
    "            \"Source\": \"source\",\n",
    "            \"Duration\": \"duration\"\n",
    "        })\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def remove_nulls(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.dropna(subset=[\"source\", \"duration\", \"episodes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execução é encapsulada em funções, para evitar o gasto de memória com os processamentos intermediários. Assim como no caso anterior, o garbage collector é explicitamente invocado para garantir a não permanência de lixo em memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_anime_analysis():\n",
    "    anime_reader = AnimeReader(ANIME_DATA_READ, ANIME_DATA_SAVE)\n",
    "\n",
    "    df_anime = anime_reader.first_process()\n",
    "    stats = anime_reader.get_stats(\n",
    "        df_anime,\n",
    "        [\"genres\", \"episodes\", \"source\", \"duration\"]\n",
    "    )\n",
    "    anime_reader.show_stats(stats)\n",
    "    df_anime = anime_reader.remove_nulls(df_anime)\n",
    "    anime_reader.to_parquet(df_anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_anime_analysis()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O leitor de classificações é responsável por criar os *datasets* prontos para o treinamento. Aqui são aplicadas as regras de negócio para corte da pontuação para gerar a variável binária de classificação e a inclusão ou não dos metadados. Ao final, são geradas 6 combinações diferentes de bases de dados, que são salvas em parquet para a utilização durante o treinamento.\n",
    "\n",
    "Neste momento é aplicado o one hot encoder para a coluna do material original, o encoder para a coluna de gêneros e a remoção das colunas de ID (após o processo de merge de dados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreReader(BaseReader):\n",
    "    def __init__(self, read_path: str, save_path: str, user_path: str, anime_path: str):\n",
    "        super().__init__(read_path, save_path)\n",
    "        self.anime_path = anime_path\n",
    "        self.user_path = user_path\n",
    "\n",
    "    def make_dataset(self, rating_cut=7, user_merge_mode=1) -> pd.DataFrame:\n",
    "        # Verifica integridade dos parâmetros\n",
    "        if rating_cut > 10 or rating_cut < 1:\n",
    "            raise Exception(\"O corte da classificação deve ser entre 1 e 10\")\n",
    "        \n",
    "        if user_merge_mode not in [1, 2]:\n",
    "            raise Exception(\"O modo de merge de usuário deve ser 1 ou 2\")\n",
    "        \n",
    "        # Carrega os dados dos scores, limpando as colunas não utilizadas\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        df = df.drop([\"Username\", \"Anime Title\"], axis=1)\n",
    "\n",
    "        # Carrega os dados de usuários e animes\n",
    "        users = pd.read_parquet(self.user_path)\n",
    "        animes = pd.read_parquet(self.anime_path)\n",
    "\n",
    "        # Recupera todos os gêneros possíveis\n",
    "        genres = [[s.strip() for s in g.split(\",\")] for g in animes[\"genres\"].values if g is not None]\n",
    "        genres: Set[str] = set(itertools.chain.from_iterable(genres))\n",
    "\n",
    "        # Define a função de verificação de gênero\n",
    "        # Os dados de gêneros são carregados como uma string,\n",
    "        # com as categorias separadas por vírgula\n",
    "        def verify_genre(genres: str | None, genre: str) -> int:\n",
    "            if genres is None:\n",
    "                return 0\n",
    "            \n",
    "            genres = [s.lower().strip() for s in genres.split(\",\")]\n",
    "            return 1 if genre.lower() in genres else 0\n",
    "\n",
    "        # Aplica o encoder para gêneros de animes\n",
    "        for genre in genres:\n",
    "            column = f\"genre_{\"_\".join(genre.lower().split(\" \"))}\"\n",
    "            animes[column] = animes[\"genres\"].apply(lambda x: verify_genre(x, genre))\n",
    "        animes = animes.drop([\"genres\"], axis=1)\n",
    "\n",
    "        # Define um encoder para o material original do anime\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "        encoder.fit(animes[[\"source\"]])\n",
    "\n",
    "        # Atualiza os dados de anime com o encoder de material original\n",
    "        encoder_df = pd.DataFrame(\n",
    "            encoder.transform(animes[[\"source\"]]),\n",
    "            columns=encoder.get_feature_names_out()\n",
    "        )\n",
    "        animes = pd.concat((animes, encoder_df), axis=1)\n",
    "        animes = animes.drop([\"source\"], axis=1)\n",
    "\n",
    "        # Executa o merge com os dados de usuários\n",
    "        # user_merge_mode = 1 faz com que apenas os dados básicos sejam usados\n",
    "        # user_merge_mode = 2 utiliza todos os dados de usuários\n",
    "        if user_merge_mode == 1:\n",
    "            users = users[[\"user_id\", \"gender\", \"age\"]]\n",
    "\n",
    "        if user_merge_mode == 2:\n",
    "            users = users[[\"user_id\", \"gender\", \"age\", \"days_spent_with_anime\", \"total_anime_watched\", \"dropped_anime\", \"mean_score\"]]\n",
    "        \n",
    "        df = df.merge(users, how=\"inner\", on=\"user_id\")\n",
    "\n",
    "        # Executa o merge com os dados de animes\n",
    "        df = df.merge(animes, how=\"inner\", on=\"anime_id\")\n",
    "\n",
    "        # Faz a criação da coluna target\n",
    "        df[\"target\"] = df[\"rating\"].apply(lambda x: 1 if x > rating_cut else 0)\n",
    "        df = df.drop([\"rating\"], axis=1)\n",
    "\n",
    "        # Finaliza o processo, removendo colunas de ID\n",
    "        df = df.drop([\"user_id\", \"anime_id\"], axis=1)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função a seguir implementa a lógica de geração de dados. Novamente, como o dataframe utilizado é relativamente grande, é tomado o cuidado para explicitamente remover a variável da memória, com a sequente chamada do garbage collector para garantir a não permanência de lixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets():\n",
    "    result_files = [\n",
    "        (FINAL_DATASET_CUT6_BASIC_USER_DATA, 6, 1),\n",
    "        (FINAL_DATASET_CUT7_BASIC_USER_DATA, 7, 1),\n",
    "        (FINAL_DATASET_CUT8_BASIC_USER_DATA, 8, 1),\n",
    "        (FINAL_DATASET_CUT6_FULL_USER_DATA, 6, 2),\n",
    "        (FINAL_DATASET_CUT7_FULL_USER_DATA, 7, 2),\n",
    "        (FINAL_DATASET_CUT8_FULL_USER_DATA, 8, 2)\n",
    "    ]\n",
    "\n",
    "    for save_path, cut, mode in result_files:\n",
    "        score_reader = ScoreReader(\n",
    "            SCORE_DATA_READ,\n",
    "            save_path,\n",
    "            USER_DATA_SAVE,\n",
    "            ANIME_DATA_SAVE\n",
    "        )\n",
    "        scores = score_reader.make_dataset(rating_cut=cut, user_merge_mode=mode)\n",
    "        score_reader.to_parquet(scores)\n",
    "\n",
    "        del scores\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_datasets()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "\n",
    "Para a execução dos experimentos são utilizados 2 modelos MLP, um com 4 camadas ocultas e outro com 8 camadas ocultas, com o intuito de verificar o comportamento dos dados quando se altera a complexidade do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4Layers(nn.Module):\n",
    "    def __init__(self, n_features: int, n_classes=2, n_neurons=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_neurons)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc4 = nn.Linear(n_neurons, n_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.out = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Model8Layers(nn.Module):\n",
    "    def __init__(self, n_features: int, n_classes=2, n_neurons=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_neurons)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc3 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc4 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc5 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc6 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc7 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc8 = nn.Linear(n_neurons, n_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.out = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe de gerenciamento encpasula o processo de amostragem de dados, treinamento e avaliação das métricas de desempenho. A amostragem é realizada por dois motivos: reduzir o tempo de processamento e o gasto de memória e garantir certa aleatoriedade aos experimentos, uma vez que não é implementada uma validação cruzada.\n",
    "\n",
    "Novamente, como os recursos de memória e tempo são escassos para este projeto, a validação cruzada se torna um ofensor junto do tamanho dos dados em memória para realizar o treinamento. Como log, os pesos dos modelos são salvos e as métricas encontradas também, registradas junto do log de execução dos experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manager:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_dataset(self, read_path: str, sample=0.2):\n",
    "        # Carrega dados processados\n",
    "        df = pd.read_parquet(read_path).sample(frac=sample)\n",
    "        X = df.drop([\"target\"], axis=1).values\n",
    "        y = df[\"target\"].values\n",
    "\n",
    "        # Faz a divisão entre treino e teste\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        # Aplica a padronização de valores\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Faz a transformação de numpy array para tensor\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        # Instancia dataset de tensores\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "        # Instancia loader de tensores\n",
    "        train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "        return train_dataset, test_dataset, train_loader, test_loader\n",
    "    \n",
    "    def execute(self, train_dataset: DataLoader, train_loader: DataLoader, epochs=100, n_neurons=16, arch=1):\n",
    "        # Garante consistência da arquitetura\n",
    "        if arch not in [1, 2]:\n",
    "            raise Exception(\"As arquiteturas válidas são 1 e 2\")\n",
    "        \n",
    "        # Regitra o tempo de início do treinamento\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Carrega dados e inicializa modelo\n",
    "        classes_ = len(train_dataset.tensors[1].unique())\n",
    "        if arch == 1:\n",
    "            model = Model4Layers(\n",
    "                n_features=train_dataset.tensors[0].shape[1],\n",
    "                n_classes=classes_,\n",
    "                n_neurons=n_neurons\n",
    "            )\n",
    "        elif arch == 2:\n",
    "            model = Model8Layers(\n",
    "                n_features=train_dataset.tensors[0].shape[1],\n",
    "                n_classes=classes_,\n",
    "                n_neurons=n_neurons\n",
    "            )\n",
    "\n",
    "        # Define o modo de otimização\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(0, epochs):\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            model.train()\n",
    "            count_batch = 0\n",
    "            limit_batch = (train_dataset.tensors[0].shape[0] // train_loader.batch_size) + 1\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                percent = round(count_batch * 100 / limit_batch, 2)\n",
    "                print(f\"Epoch {epoch + 1} Batch {count_batch + 1} ({percent}%)\", end=\"\\r\")\n",
    "                inputs = inputs\n",
    "                labels = labels\n",
    "\n",
    "                # Inicia os gradientes e calcula a predição\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                pred_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                # Teoricamente, seria preciso passar as labels do dataset para o\n",
    "                # padrão one hot encoder, porém a camada softmax no modelo já\n",
    "                # resolve isso.\n",
    "                # oh_labels = F.one_hot(labels.long())\n",
    "                # loss = criterion(outputs, torch.reshape(oh_labels, (oh_labels.size()[0], classes_)).float())\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Calcula os gradientes e atualiza os pesos\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Fal a atualização das estatísticas de acompanhamento\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(pred_labels == labels.data).item()\n",
    "                count_batch = count_batch + 1\n",
    "\n",
    "            # Exibe estatísticas de acompanhamento\n",
    "            num_samples = len(train_dataset)\n",
    "            epoch_loss = running_loss / num_samples\n",
    "            epoch_accuracy = running_corrects / num_samples\n",
    "            print(f\"Epoch {epoch + 1}: Loss {epoch_loss:.3f} Acurácia {epoch_accuracy:.3f}\")\n",
    "\n",
    "        # Calcula o tempo de execução do treinamento\n",
    "        end_time = time.time()\n",
    "        train_time = end_time - start_time\n",
    "\n",
    "        return model, train_time, epoch_loss\n",
    "    \n",
    "    def compute_test(self, model: nn.Module, test_loader: DataLoader, train_time: float, train_loss: float) -> dict:\n",
    "        # Define modelo como avaliação e inicia as listas de labels\n",
    "        model.eval()\n",
    "        pred_labels_all = []\n",
    "        true_labels_all = []\n",
    "\n",
    "        # Passa pelo loader para cálculo das predições\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs\n",
    "            outputs = model(inputs)\n",
    "            pred_labels = torch.argmax(outputs, dim=1)\n",
    "            pred_labels_all.append(pred_labels)\n",
    "            true_labels_all.append(labels)\n",
    "\n",
    "        # Concatena os resultados\n",
    "        pred_labels = torch.cat(pred_labels_all, dim=0).cpu().numpy()\n",
    "        true_labels = torch.cat(true_labels_all, dim=0).numpy()\n",
    "\n",
    "        # Registra dados no dicionário de dados\n",
    "        return {\n",
    "            \"train_time\": train_time,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"metrics\": {\n",
    "                \"accuracy\": (pred_labels == true_labels).mean(),\n",
    "                \"f1-score\": f1_score(true_labels, pred_labels, pos_label=1, average=\"binary\"),\n",
    "                \"recall-score\": recall_score(true_labels, pred_labels, pos_label=1, average=\"binary\"),\n",
    "                \"precission-score\": precision_score(true_labels, pred_labels, pos_label=1, average=\"binary\")\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os experimentos são automatizados por meio desta função, que se aproveita dos logs para recuperar o último estágio de treinamento, garantindo a capacidade de executar os experimentos mesmo que sejam interrompidos uma vez. Isto garante flexibilidade para executar os experimentos em momentos distintos e também certa resistência à falha de máquina por um motivo qualquer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_experiments():\n",
    "    # Define os parâmetros dos experimentos\n",
    "    archs_set = [1, 2]\n",
    "    neurons_set = [16]\n",
    "    sample_data = [0.1, 0.2]\n",
    "    epochs = [10]\n",
    "    repeat = 5\n",
    "    data_paths = [\n",
    "        FINAL_DATASET_CUT6_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT7_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT8_BASIC_USER_DATA,\n",
    "        FINAL_DATASET_CUT6_FULL_USER_DATA,\n",
    "        FINAL_DATASET_CUT7_FULL_USER_DATA,\n",
    "        FINAL_DATASET_CUT8_FULL_USER_DATA\n",
    "    ]\n",
    "\n",
    "    # Verifica o log de experimentos\n",
    "    if not os.path.exists(EXPERIMENT_LOG):\n",
    "        with open(EXPERIMENT_LOG, \"w\") as file:\n",
    "            file.write(\"dataset_type,arch,neurons,sample,epochs,iteration,weight_file,predict_file\\n\")\n",
    "\n",
    "    # Função auxiliar: abre o log e verifica registros\n",
    "    def verify(dataset_type: str, arch: int, neurons: int, sample: float, epochs: int, iteration: int):\n",
    "        exist = False\n",
    "        with open(EXPERIMENT_LOG, \"r\") as file:\n",
    "            row = file.readline()\n",
    "            while row:\n",
    "                row_dataset_type, row_arch, row_neurons, row_sample, row_epochs, row_iteration, _, _ = row.split(\",\")\n",
    "                row_params = [row_dataset_type, row_arch, row_neurons, row_sample, row_epochs, row_iteration]\n",
    "                search_params = [str(dataset_type), str(arch), str(neurons), str(sample), str(epochs), str(iteration)]\n",
    "                \n",
    "                if row_params == search_params:\n",
    "                    exist = True\n",
    "                    break\n",
    "\n",
    "                row = file.readline()\n",
    "\n",
    "        return exist\n",
    "\n",
    "    for data_path in data_paths:\n",
    "        # Tipo de dataset utilizado\n",
    "        dataset_type = data_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        for neurons in neurons_set:\n",
    "            # Quantidade de neurônios das camadas internas\n",
    "\n",
    "            for sample in sample_data:\n",
    "                # Porção dos dados fracionados\n",
    "\n",
    "                for epoch in epochs:\n",
    "                    # Quantidade de épocas do treinamento\n",
    "\n",
    "                    for arch in archs_set:\n",
    "                        # Profundidade da rede\n",
    "\n",
    "                        for i in range(0, repeat):\n",
    "                            # Verifica se o experimento já foi executado\n",
    "                            if verify(dataset_type, arch, neurons, sample, epoch, i):\n",
    "                                continue\n",
    "\n",
    "                            # Registra todos os dados do experimento\n",
    "                            unique_name = int(time.time())\n",
    "                            weight_path = f\"{RESULTS_DIR}/{unique_name}.pth\"\n",
    "                            predict_path = f\"{RESULTS_DIR}/{unique_name}.json\"\n",
    "                            experiment_data = [\n",
    "                                    dataset_type,\n",
    "                                    str(arch),\n",
    "                                    str(neurons),\n",
    "                                    str(sample),\n",
    "                                    str(epoch),\n",
    "                                    str(i),\n",
    "                                    f\"{unique_name}.pth\",\n",
    "                                    f\"{unique_name}.json\"\n",
    "                                ]\n",
    "\n",
    "                            # Log de execução\n",
    "                            print(f\"Execução do experimento {\",\".join(experiment_data[:-2])}\".upper())\n",
    "\n",
    "                            # Repetição do experimento\n",
    "                            process = Manager()\n",
    "                            train_dataset, test_dataset, train_loader, test_loader = process.get_dataset(data_path, sample=sample)\n",
    "                            gc.collect()\n",
    "                            model, train_time, train_loss = process.execute(train_dataset, train_loader, epochs=epoch, n_neurons=neurons, arch=arch)\n",
    "\n",
    "                            # Executa o teste do modelo\n",
    "                            results = process.compute_test(model, test_loader, train_time, train_loss)\n",
    "                            \n",
    "                            # Salva o json de métricas\n",
    "                            with open(predict_path, \"w+\") as file:\n",
    "                                file.write(json.dumps(results))\n",
    "\n",
    "                            # Salva os pesos do modelo\n",
    "                            torch.save(model.state_dict(), weight_path)\n",
    "\n",
    "                            # Registra no log\n",
    "                            with open(EXPERIMENT_LOG, \"a\") as file:\n",
    "                                file.write(\",\".join(experiment_data) + \"\\n\")\n",
    "\n",
    "                            # Libera memória\n",
    "                            del train_dataset, test_dataset, train_loader, test_loader\n",
    "                            gc.collect()\n",
    "                            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise\n",
    "\n",
    "Como principal necessidade é preciso calcular as médias junto do teste estatístico de média, categorizado por cada fator (tratamento) dos experimentos. Esta função genérica é implementada para garantir esta possibilidade, permitindo que se definas as colunas que caracterizam o fator e a coluna de análise. Isto permite maior limpeza e replicabilidade da análise.\n",
    "\n",
    "O padrão de análise é sobre o F1-score, porém é possível alterar para qualquer métrica presente no log dos experimentos. Todavia, para simplificações, o F1-score foi escolhido como a métrica utilizada neste trabalho exploratório, justamente por ser uma média harmônica entre revocação e precisão. Já para o teste de média, foi utilizado o test t, já disponível na biblioteca scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis(\n",
    "    df: pd.DataFrame,\n",
    "    factor_columns: List[str],\n",
    "    analysis_column: str,\n",
    "    metrics_dir: str,\n",
    "    metric_name=\"f1-score\"\n",
    "):\n",
    "    # Instancia o dataset de resposta\n",
    "    df_result = []\n",
    "\n",
    "    # Faz a criação dos fatores da execução\n",
    "    factors: List[List[Tuple[str, int|float|str]]] = []\n",
    "    values = dict()\n",
    "\n",
    "    for column in factor_columns:\n",
    "        values[column] = []\n",
    "        for value in df[column].unique():\n",
    "            values[column].append((column, value))\n",
    "\n",
    "    factors = list(itertools.product(*values.values()))\n",
    "\n",
    "    # Para cada fator, cria o subset de análise\n",
    "    for factor_list in factors:\n",
    "        factor_name = []\n",
    "        subset = df\n",
    "\n",
    "        for column, value in factor_list:\n",
    "            subset = subset.loc[subset[column] == value]\n",
    "            factor_name.append(f\"{column}={str(value)}\")\n",
    "        factor_name = \",\".join(factor_name)\n",
    "\n",
    "        # Encontra a quantidade de elementos da análise\n",
    "        elements = df[analysis_column].unique()\n",
    "        register = {\"factor\": factor_name}\n",
    "\n",
    "        # Percorre o conjunto filtrado pelo fator\n",
    "        for _, row in subset.iterrows():\n",
    "\n",
    "            # Acessa o arquivo de métricas\n",
    "            with open(f\"{metrics_dir}/{row[\"predict_file\"]}\") as file:\n",
    "                result = json.load(file)\n",
    "\n",
    "            # Verifica qual item de análise será utilizado\n",
    "            for e in elements:\n",
    "                if row[analysis_column] == e:\n",
    "                    if e not in register.keys():\n",
    "                        register[e] = []\n",
    "                    register[e].append(result[\"metrics\"][metric_name])\n",
    "                    break\n",
    "        \n",
    "        # Executa o teste de média para cada combinação\n",
    "        combinations = list(itertools.combinations(elements, 2))\n",
    "        for column1, column2 in combinations:\n",
    "            register[f\"ttest-{column1}-{column2}\"] = stats.ttest_ind(\n",
    "                register[column1], register[column2]\n",
    "            ).pvalue\n",
    "\n",
    "        # Converte as listas para a média\n",
    "        for e in elements:\n",
    "            register[e] = np.mean(register[e])\n",
    "\n",
    "        # Salva os dados no dataset de resultados\n",
    "        df_result.append(register)\n",
    "\n",
    "    return pd.DataFrame(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto de dados dos experimentos é carregado de modo global, garantindo maior facilidade no uso posterior e, como ele não ocupa muito espaço em memória, não se espera um grande impacto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(EXPERIMENT_LOG)\n",
    "df[\"cut\"] = df[\"dataset_type\"].apply(lambda x: x.split(\"-\")[1])\n",
    "df[\"features\"] = df[\"dataset_type\"].apply(lambda x: x.split(\"-\")[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionamento: Existe ganho ao se incluir metadados dos usuários?\n",
    "\n",
    "Uma das primeiras hipóteses é de que a inclusão dos metadados dos usuários aumentaria a capacidade preditiva do modelo. De fato, na maior parte dos fatores analisados isto ocorre, em alguns de modo mais expressivo e em outros de modo menos expressivo. Porém, o teste de média garante que para todos os fatores as médias são diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor</th>\n",
       "      <th>basic</th>\n",
       "      <th>full</th>\n",
       "      <th>ttest-basic-full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arch=1,sample=0.1,cut=cut6</td>\n",
       "      <td>0.888312</td>\n",
       "      <td>0.895944</td>\n",
       "      <td>1.124163e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arch=1,sample=0.1,cut=cut7</td>\n",
       "      <td>0.708368</td>\n",
       "      <td>0.762878</td>\n",
       "      <td>1.958774e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arch=1,sample=0.1,cut=cut8</td>\n",
       "      <td>0.311931</td>\n",
       "      <td>0.536893</td>\n",
       "      <td>4.948791e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arch=1,sample=0.2,cut=cut6</td>\n",
       "      <td>0.888384</td>\n",
       "      <td>0.896093</td>\n",
       "      <td>5.701522e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arch=1,sample=0.2,cut=cut7</td>\n",
       "      <td>0.705443</td>\n",
       "      <td>0.761860</td>\n",
       "      <td>1.809408e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>arch=1,sample=0.2,cut=cut8</td>\n",
       "      <td>0.309045</td>\n",
       "      <td>0.555534</td>\n",
       "      <td>1.653350e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arch=2,sample=0.1,cut=cut6</td>\n",
       "      <td>0.887408</td>\n",
       "      <td>0.896105</td>\n",
       "      <td>9.693076e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>arch=2,sample=0.1,cut=cut7</td>\n",
       "      <td>0.706057</td>\n",
       "      <td>0.762408</td>\n",
       "      <td>1.088779e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>arch=2,sample=0.1,cut=cut8</td>\n",
       "      <td>0.280773</td>\n",
       "      <td>0.541882</td>\n",
       "      <td>1.329206e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>arch=2,sample=0.2,cut=cut6</td>\n",
       "      <td>0.888257</td>\n",
       "      <td>0.896011</td>\n",
       "      <td>1.944084e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>arch=2,sample=0.2,cut=cut7</td>\n",
       "      <td>0.705468</td>\n",
       "      <td>0.762367</td>\n",
       "      <td>3.437826e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>arch=2,sample=0.2,cut=cut8</td>\n",
       "      <td>0.303647</td>\n",
       "      <td>0.550261</td>\n",
       "      <td>1.370115e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        factor     basic      full  ttest-basic-full\n",
       "0   arch=1,sample=0.1,cut=cut6  0.888312  0.895944      1.124163e-07\n",
       "1   arch=1,sample=0.1,cut=cut7  0.708368  0.762878      1.958774e-08\n",
       "2   arch=1,sample=0.1,cut=cut8  0.311931  0.536893      4.948791e-09\n",
       "3   arch=1,sample=0.2,cut=cut6  0.888384  0.896093      5.701522e-10\n",
       "4   arch=1,sample=0.2,cut=cut7  0.705443  0.761860      1.809408e-09\n",
       "5   arch=1,sample=0.2,cut=cut8  0.309045  0.555534      1.653350e-09\n",
       "6   arch=2,sample=0.1,cut=cut6  0.887408  0.896105      9.693076e-08\n",
       "7   arch=2,sample=0.1,cut=cut7  0.706057  0.762408      1.088779e-07\n",
       "8   arch=2,sample=0.1,cut=cut8  0.280773  0.541882      1.329206e-08\n",
       "9   arch=2,sample=0.2,cut=cut6  0.888257  0.896011      1.944084e-08\n",
       "10  arch=2,sample=0.2,cut=cut7  0.705468  0.762367      3.437826e-08\n",
       "11  arch=2,sample=0.2,cut=cut8  0.303647  0.550261      1.370115e-11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(\n",
    "    df,\n",
    "    factor_columns=[\"arch\", \"sample\", \"cut\"],\n",
    "    analysis_column=\"features\",\n",
    "    metrics_dir=RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionamento: A forma como definir o limite entre gostou ou não gostou gera impacto no resultado?\n",
    "\n",
    "Uma decisão de negócio que se estimava impactar no desempenho do modelo é a forma de se definir a variável alvo binária de gostou ou não gostou da obra. A base original é composta por scores de 1 até 10 e foram utilizadas três regras diferentes para conversão em binária:\n",
    "\n",
    "- cut6: para valores maiores do que 6, considera-se que o usuário gostou do anime;\n",
    "- cut7: para valores maiores do que 7, considera-se que o usuário gostou do anime;\n",
    "- cut8: para valores maiores do que 8, considera-se que o usuário gostou do anime.\n",
    "\n",
    "Para a regra cut6 foram encontrados os maiores valores, porém esta configuração gera um desbalanceamento das classes, com cerca de 80% dos registros marcados como gostei. Isto gera uma dúvida sobre o impacto do balanceamento na capacidade preditiva do modelo, que precisará ser explorado com mais afinco futuramente.\n",
    "\n",
    "Importante ainda comentar que, para a regra cut8, que é bem mais exigente sobre o gostar ou não gostar, o modelo apresenta os piores resultados. Novamente tem-se uma evidência que que o MLP proposto não consegue lidar bem com o grande desbalanceamento das classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_dist(read_paths: List[str]) -> None:\n",
    "    for read_path in read_paths:\n",
    "        df = pd.read_parquet(read_path)\n",
    "\n",
    "        print(f\"Distribuição da target em {read_path}\")\n",
    "        for target, count in df[\"target\"].value_counts().items():\n",
    "            percent = round(count * 100 / len(df), 2)\n",
    "            print(f\"{target}: {percent}%\")\n",
    "        print(\"\")\n",
    "\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor</th>\n",
       "      <th>cut6</th>\n",
       "      <th>cut7</th>\n",
       "      <th>cut8</th>\n",
       "      <th>ttest-cut6-cut7</th>\n",
       "      <th>ttest-cut6-cut8</th>\n",
       "      <th>ttest-cut7-cut8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arch=1,sample=0.1,features=basic</td>\n",
       "      <td>0.888312</td>\n",
       "      <td>0.708368</td>\n",
       "      <td>0.311931</td>\n",
       "      <td>1.207986e-13</td>\n",
       "      <td>7.911281e-14</td>\n",
       "      <td>2.349218e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arch=1,sample=0.1,features=full</td>\n",
       "      <td>0.895944</td>\n",
       "      <td>0.762878</td>\n",
       "      <td>0.536893</td>\n",
       "      <td>9.855015e-13</td>\n",
       "      <td>1.485798e-11</td>\n",
       "      <td>7.582376e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arch=1,sample=0.2,features=basic</td>\n",
       "      <td>0.888384</td>\n",
       "      <td>0.705443</td>\n",
       "      <td>0.309045</td>\n",
       "      <td>1.720762e-14</td>\n",
       "      <td>1.496629e-12</td>\n",
       "      <td>3.513435e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arch=1,sample=0.2,features=full</td>\n",
       "      <td>0.896093</td>\n",
       "      <td>0.761860</td>\n",
       "      <td>0.555534</td>\n",
       "      <td>6.473266e-14</td>\n",
       "      <td>7.305112e-16</td>\n",
       "      <td>1.794684e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arch=2,sample=0.1,features=basic</td>\n",
       "      <td>0.887408</td>\n",
       "      <td>0.706057</td>\n",
       "      <td>0.280773</td>\n",
       "      <td>8.150079e-12</td>\n",
       "      <td>1.208087e-11</td>\n",
       "      <td>2.788426e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>arch=2,sample=0.1,features=full</td>\n",
       "      <td>0.896105</td>\n",
       "      <td>0.762408</td>\n",
       "      <td>0.541882</td>\n",
       "      <td>4.742614e-15</td>\n",
       "      <td>3.858148e-14</td>\n",
       "      <td>2.026072e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arch=2,sample=0.2,features=basic</td>\n",
       "      <td>0.888257</td>\n",
       "      <td>0.705468</td>\n",
       "      <td>0.303647</td>\n",
       "      <td>1.802789e-14</td>\n",
       "      <td>2.340404e-15</td>\n",
       "      <td>8.447495e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>arch=2,sample=0.2,features=full</td>\n",
       "      <td>0.896011</td>\n",
       "      <td>0.762367</td>\n",
       "      <td>0.550261</td>\n",
       "      <td>1.199389e-11</td>\n",
       "      <td>1.643081e-14</td>\n",
       "      <td>7.672061e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             factor      cut6      cut7      cut8  \\\n",
       "0  arch=1,sample=0.1,features=basic  0.888312  0.708368  0.311931   \n",
       "1   arch=1,sample=0.1,features=full  0.895944  0.762878  0.536893   \n",
       "2  arch=1,sample=0.2,features=basic  0.888384  0.705443  0.309045   \n",
       "3   arch=1,sample=0.2,features=full  0.896093  0.761860  0.555534   \n",
       "4  arch=2,sample=0.1,features=basic  0.887408  0.706057  0.280773   \n",
       "5   arch=2,sample=0.1,features=full  0.896105  0.762408  0.541882   \n",
       "6  arch=2,sample=0.2,features=basic  0.888257  0.705468  0.303647   \n",
       "7   arch=2,sample=0.2,features=full  0.896011  0.762367  0.550261   \n",
       "\n",
       "   ttest-cut6-cut7  ttest-cut6-cut8  ttest-cut7-cut8  \n",
       "0     1.207986e-13     7.911281e-14     2.349218e-12  \n",
       "1     9.855015e-13     1.485798e-11     7.582376e-10  \n",
       "2     1.720762e-14     1.496629e-12     3.513435e-11  \n",
       "3     6.473266e-14     7.305112e-16     1.794684e-13  \n",
       "4     8.150079e-12     1.208087e-11     2.788426e-10  \n",
       "5     4.742614e-15     3.858148e-14     2.026072e-12  \n",
       "6     1.802789e-14     2.340404e-15     8.447495e-14  \n",
       "7     1.199389e-11     1.643081e-14     7.672061e-12  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(\n",
    "    df,\n",
    "    factor_columns=[\"arch\", \"sample\", \"features\"],\n",
    "    analysis_column=\"cut\",\n",
    "    metrics_dir=RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição da target em /media/bruno/Arquivos/Desenvolvimento/annproject/data/scores-cut6-basic.parquet\n",
      "1: 79.86%\n",
      "0: 20.14%\n",
      "\n",
      "Distribuição da target em /media/bruno/Arquivos/Desenvolvimento/annproject/data/scores-cut7-basic.parquet\n",
      "1: 57.71%\n",
      "0: 42.29%\n",
      "\n",
      "Distribuição da target em /media/bruno/Arquivos/Desenvolvimento/annproject/data/scores-cut8-basic.parquet\n",
      "0: 67.5%\n",
      "1: 32.5%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_target_dist([\n",
    "    FINAL_DATASET_CUT6_BASIC_USER_DATA,\n",
    "    FINAL_DATASET_CUT7_BASIC_USER_DATA,\n",
    "    FINAL_DATASET_CUT8_BASIC_USER_DATA\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionamento: O aumento da profundidade do MLP melhora o resultado preditivo?\n",
    "\n",
    "É visível que a métrica não se altera para as diferentes profundidades, o que pode ser confirmado pelo resultado do teste t, cujo valor p é alto demais para rejeitar a hipótese nula (as médias são iguais.).\n",
    "\n",
    "Vale comentar sobre os fatores 0 e 2, relativos às regras cut6 e cut8, que estatisticamente podem ter seus resultados classificados como diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>ttest-1-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample=0.1,features=basic,cut=cut6</td>\n",
       "      <td>0.888312</td>\n",
       "      <td>0.887408</td>\n",
       "      <td>0.016724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample=0.1,features=basic,cut=cut7</td>\n",
       "      <td>0.708368</td>\n",
       "      <td>0.706057</td>\n",
       "      <td>0.536916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample=0.1,features=basic,cut=cut8</td>\n",
       "      <td>0.311931</td>\n",
       "      <td>0.280773</td>\n",
       "      <td>0.034157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample=0.1,features=full,cut=cut6</td>\n",
       "      <td>0.895944</td>\n",
       "      <td>0.896105</td>\n",
       "      <td>0.787576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample=0.1,features=full,cut=cut7</td>\n",
       "      <td>0.762878</td>\n",
       "      <td>0.762408</td>\n",
       "      <td>0.808731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sample=0.1,features=full,cut=cut8</td>\n",
       "      <td>0.536893</td>\n",
       "      <td>0.541882</td>\n",
       "      <td>0.513080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sample=0.2,features=basic,cut=cut6</td>\n",
       "      <td>0.888384</td>\n",
       "      <td>0.888257</td>\n",
       "      <td>0.553339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sample=0.2,features=basic,cut=cut7</td>\n",
       "      <td>0.705443</td>\n",
       "      <td>0.705468</td>\n",
       "      <td>0.990722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sample=0.2,features=basic,cut=cut8</td>\n",
       "      <td>0.309045</td>\n",
       "      <td>0.303647</td>\n",
       "      <td>0.556095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sample=0.2,features=full,cut=cut6</td>\n",
       "      <td>0.896093</td>\n",
       "      <td>0.896011</td>\n",
       "      <td>0.826883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sample=0.2,features=full,cut=cut7</td>\n",
       "      <td>0.761860</td>\n",
       "      <td>0.762367</td>\n",
       "      <td>0.854994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sample=0.2,features=full,cut=cut8</td>\n",
       "      <td>0.555534</td>\n",
       "      <td>0.550261</td>\n",
       "      <td>0.143383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                factor         1         2  ttest-1-2\n",
       "0   sample=0.1,features=basic,cut=cut6  0.888312  0.887408   0.016724\n",
       "1   sample=0.1,features=basic,cut=cut7  0.708368  0.706057   0.536916\n",
       "2   sample=0.1,features=basic,cut=cut8  0.311931  0.280773   0.034157\n",
       "3    sample=0.1,features=full,cut=cut6  0.895944  0.896105   0.787576\n",
       "4    sample=0.1,features=full,cut=cut7  0.762878  0.762408   0.808731\n",
       "5    sample=0.1,features=full,cut=cut8  0.536893  0.541882   0.513080\n",
       "6   sample=0.2,features=basic,cut=cut6  0.888384  0.888257   0.553339\n",
       "7   sample=0.2,features=basic,cut=cut7  0.705443  0.705468   0.990722\n",
       "8   sample=0.2,features=basic,cut=cut8  0.309045  0.303647   0.556095\n",
       "9    sample=0.2,features=full,cut=cut6  0.896093  0.896011   0.826883\n",
       "10   sample=0.2,features=full,cut=cut7  0.761860  0.762367   0.854994\n",
       "11   sample=0.2,features=full,cut=cut8  0.555534  0.550261   0.143383"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(\n",
    "    df,\n",
    "    factor_columns=[\"sample\", \"features\", \"cut\"],\n",
    "    analysis_column=\"arch\",\n",
    "    metrics_dir=RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionamento: Aumentar a quantidade de dados amostrados aumenta o desempenho do modelo?\n",
    "\n",
    "É comum pensar em aumentar a quantidade de dados de treinamento para aumentar o desempenho do modelo. Como por limitações de memória os experimentos são conduzidos com uma amostragem do conjunto de dados originais, pode-se realizar este tipo de teste para verificar se mais dados fazem o modelo ter maior desempenho.\n",
    "\n",
    "Novamente, as médias e o teste t não deixam evidências de que podemos considerar que existem diferenças entre os casos de diferentes quantidades de dados utilizados para treinamento. Identifica-se uma exceção nos fatores que utilizam a regra cut7 e cut8 (semelhante ao caso anterior), cujos valores do teste t podem indicar diferença entre as médias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>ttest-0.1-0.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arch=1,features=basic,cut=cut6</td>\n",
       "      <td>0.888312</td>\n",
       "      <td>0.888384</td>\n",
       "      <td>0.777809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arch=1,features=basic,cut=cut7</td>\n",
       "      <td>0.708368</td>\n",
       "      <td>0.705443</td>\n",
       "      <td>0.241177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arch=1,features=basic,cut=cut8</td>\n",
       "      <td>0.311931</td>\n",
       "      <td>0.309045</td>\n",
       "      <td>0.774331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arch=1,features=full,cut=cut6</td>\n",
       "      <td>0.895944</td>\n",
       "      <td>0.896093</td>\n",
       "      <td>0.733955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arch=1,features=full,cut=cut7</td>\n",
       "      <td>0.762878</td>\n",
       "      <td>0.761860</td>\n",
       "      <td>0.642415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>arch=1,features=full,cut=cut8</td>\n",
       "      <td>0.536893</td>\n",
       "      <td>0.555534</td>\n",
       "      <td>0.026259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arch=2,features=basic,cut=cut6</td>\n",
       "      <td>0.887408</td>\n",
       "      <td>0.888257</td>\n",
       "      <td>0.013637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>arch=2,features=basic,cut=cut7</td>\n",
       "      <td>0.706057</td>\n",
       "      <td>0.705468</td>\n",
       "      <td>0.867389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>arch=2,features=basic,cut=cut8</td>\n",
       "      <td>0.280773</td>\n",
       "      <td>0.303647</td>\n",
       "      <td>0.081461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>arch=2,features=full,cut=cut6</td>\n",
       "      <td>0.896105</td>\n",
       "      <td>0.896011</td>\n",
       "      <td>0.864736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>arch=2,features=full,cut=cut7</td>\n",
       "      <td>0.762408</td>\n",
       "      <td>0.762367</td>\n",
       "      <td>0.987429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>arch=2,features=full,cut=cut8</td>\n",
       "      <td>0.541882</td>\n",
       "      <td>0.550261</td>\n",
       "      <td>0.074701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            factor       0.1       0.2  ttest-0.1-0.2\n",
       "0   arch=1,features=basic,cut=cut6  0.888312  0.888384       0.777809\n",
       "1   arch=1,features=basic,cut=cut7  0.708368  0.705443       0.241177\n",
       "2   arch=1,features=basic,cut=cut8  0.311931  0.309045       0.774331\n",
       "3    arch=1,features=full,cut=cut6  0.895944  0.896093       0.733955\n",
       "4    arch=1,features=full,cut=cut7  0.762878  0.761860       0.642415\n",
       "5    arch=1,features=full,cut=cut8  0.536893  0.555534       0.026259\n",
       "6   arch=2,features=basic,cut=cut6  0.887408  0.888257       0.013637\n",
       "7   arch=2,features=basic,cut=cut7  0.706057  0.705468       0.867389\n",
       "8   arch=2,features=basic,cut=cut8  0.280773  0.303647       0.081461\n",
       "9    arch=2,features=full,cut=cut6  0.896105  0.896011       0.864736\n",
       "10   arch=2,features=full,cut=cut7  0.762408  0.762367       0.987429\n",
       "11   arch=2,features=full,cut=cut8  0.541882  0.550261       0.074701"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_analysis(\n",
    "    df,\n",
    "    factor_columns=[\"arch\", \"features\", \"cut\"],\n",
    "    analysis_column=\"sample\",\n",
    "    metrics_dir=RESULTS_DIR\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
